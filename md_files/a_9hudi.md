# 个人理解

```mysql
#误区
1 不要认为hive执行慢，就认为hdfs插入和读取慢。hbase能基于hdfs做数据库，说明读取数据完全没问题，都是毫秒级别
2 hudi不是数据库,要进行数据分析，需要用spark引擎写spark代码，doris是一个mpp数据库和clickhous一样

#hive功能缺陷
1 因为hdfs原因，hive不支持更新和删除，导致hive更新和删除乏力
2 hql是基于mr处理速度是分钟级别,想实时直接写入hive的表数据，那么需要满足hive 表数据的格式要求(需要对hive框架很了解)，
	并且hive	对于外部插入的文件，需要执行load指令。
	
#hudi
基于hive的原因，hudi的改进就是支持更新和删除，并且外部写入数据不是基于mr，可以达到毫秒级别写入，并且写入的数据能实时读取
	

```



# Hudi简介

```
Hive 和 Hudi 是两个不同的项目，它们都是在大数据生态系统中使用的，但它们的目的和功能有所不同。
Hive 是一个建立在 Hadoop 上的数据仓库软件，用于数据摘要、查询和分析。Hive 使得那些熟悉 SQL 的人可以通过 HiveQL（一种类似于 SQL 的查询语言）编写查询。这些查询然后被转换成一系列的 MapReduce、Tez 或 Spark 作业来执行。Hive 旨在提供一个 SQL 接近的接口来查询存储在 Hadoop 文件系统（HDFS）或其他兼容存储系统中的大数据集。
Hudi（Apache Hudi）代表 "Hadoop Upserts Deletes and Incrementals"，是一个为 Hadoop 生态系统构建的数据存储层，用于支持高效的插入、更新和删除操作。Hudi 通过提供快速的增量数据处理和数据变更流功能，使得在大数据平台上进行更复杂的数据管道操作成为可能。
Hudi 的主要特点和优势包括：
支持近实时的数据插入、更新和删除：Hudi 允许对存储的数据集进行快速修改，这对于需要频繁更新数据的应用程序非常重要。
增量处理：Hudi 支持增量读取，这意味着可以仅查询自上次查询以来发生变化的数据，从而提高查询效率。
时间旅行：Hudi 提供了查看数据在任意时间点的能力，这是通过跟踪数据的变化历史来实现的。
事务支持：Hudi 提供了对多个写操作的原子性保证，这对确保数据的一致性至关重要。
最初，Hudi 被设计出来是为了解决 Uber 大数据平台上数据存储的问题，特别是在高吞吐量的数据流中快速更新大型数据集的需求。Uber 需要一种方式来有效地处理大量的数据插入、更新和删除操作，同时保持数据的一致性和查询效率。Hudi 就是为了解决这些挑战而创建的。
与 Hive 相比，Hudi 提供了更为高级的数据管理功能，特别是对于需要处理数据变更和更新的场景。而 Hive 更多地被用于静态数据的批量处理和分析。两者可以结合使用，在 Hudi 管理的数据上运行 Hive 查询，以充分利用 Hive 的 SQL 查询能力和 Hudi 的高效数据管理。
```



# 对比hive+hadoop

```mysql
传统的离线数据仓库架构（Hive + MapReduce + HDFS）和现代的Spark + Hudi架构在很多方面都有所不同。下面分别解释这两种架构，并讨论它们的差异和适用场景。

### 传统的Hive + MapReduce + HDFS架构：

1. **Hive**：是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供完整的SQL查询功能。

2. **MapReduce**：是Hadoop的编程模型，用于处理大规模数据集的并行运算。但是MapReduce的处理速度较慢，因为它在每个任务之间需要写入和读取磁盘。

3. **HDFS**：是Hadoop分布式文件系统，设计用于存储大量数据，并提供高吞吐量的数据访问。

这种架构的特点是稳定性和成熟度高，但是在处理速度、实时查询以及数据更新和插入方面存在局限性。MapReduce计算模型在处理大数据时通常比较慢，且不适合需要快速迭代的数据处理任务。

### Spark + Hudi架构：

1. **Spark**：是一个快速的通用的大数据处理引擎。它提供了一个更高级的抽象（RDDs、DataFrames、Datasets），可以进行内存计算，从而大大加快了数据处理速度。

2. **Hudi**：（Apache Hudi）是一个开源的数据湖框架，用于管理存储在分布式文件系统中的大规模数据集。Hudi提供了记录级别的插入、更新和删除操作，以及更改数据捕获（CDC）、事务、快照和增量拉取等功能。

这种架构的特点是计算速度快，可以处理实时或近实时的数据处理和分析任务。Spark优化了内存计算，减少了磁盘I/O操作，而Hudi则允许对数据进行更灵活的操作，如更新和删除，这在传统的Hive + MapReduce架构中是很难实现的。

### 差别：

- **性能**：Spark比MapReduce快很多，特别是在需要多次迭代的计算任务中，因为Spark可以将中间数据保留在内存中。
- **实时处理**：Spark可以进行实时数据处理，而Hive + MapReduce更适合批量处理。
- **数据更新**：Hudi支持对数据的增量更新和删除，而传统的Hive架构不支持或者效率很低。
- **易用性**：Spark提供了更高级的API，对于开发者来说更容易编写和优化程序。

### 适用场景：

- 当需要处理大规模的批量数据，对实时性要求不高，且数据不经常更新时，传统的Hive + MapReduce + HDFS架构可能更适合。
- 当需要快速处理数据，特别是需要实时分析、频繁的数据更新、插入以及删除时，Spark + Hudi架构更有优势。

总的来说，如果你的场景需要高性能、实时处理以及更复杂的数据更新策略，那么Spark + Hudi架构可能是更好的选择。随着技术的发展，越来越多的组织倾向于使用更现代、更灵活的数据处理框架来满足他们的需求。
```



# spark对接hudi的用途

```mysql
实际上，Spark + Hudi架构并不限于仅使用Spark Streaming。Apache Spark的生态系统提供了多个组件，包括Spark SQL、Spark Streaming、MLlib（机器学习库）和GraphX（图计算库）。这些组件可以与Hudi结合，以适应不同的用例和场景。下面是一些可能的组合和应用场景：

### Spark Streaming + Hudi
- **实时数据管道**：在需要实时处理流数据并将结果存储到数据湖中时，Spark Streaming + Hudi是一个强大的组合。例如，可以实时捕获事件日志、交易记录等，并利用Hudi提供的upsert（更新+插入）功能来维护数据的最新状态。

### Spark SQL + Hudi
- **交互式查询**：Spark SQL可以用来对存储在Hudi中的数据执行SQL查询，这对于数据分析和探索非常有用。因为Hudi支持快照查询和增量查询，Spark SQL可以非常方便地用来进行数据仓库的交互式数据分析。
- **批量处理**：对于批量数据处理，Spark SQL可以与Hudi结合使用，以利用其高效的数据处理能力和Hudi的数据更新和删除能力。

### Spark Core (RDD API) + Hudi
- **复杂的数据处理**：当需要执行复杂的数据转换和处理时，可以使用Spark Core的RDD API完成这些任务，并使用Hudi来存储和跟踪数据集的变更。

### Spark Structured Streaming + Hudi
- **结构化流处理**：Spark的Structured Streaming是一个流处理框架，它提供了一个高级的结构化API。与Hudi结合时，它可以用于处理来自Kafka等实时数据源的数据流，并利用Hudi的功能来实现复杂的流处理用例，如事件时间处理、窗口操作和状态管理。

因此，Spark SQL在Hudi的应用场景并不少见。实际上，Spark SQL是与Hudi一起使用时的一个非常重要的组件，因为它提供了处理数据的声明性方式，使得数据分析和管理变得更加容易和高效。在数据湖和数据仓库的场景中，Spark SQL是进行数据查询和报告的常用工具。通过结合Hudi的能力，Spark SQL可以对大规模数据集进行高效的增量读取和实时更新，这在传统的数据仓库解决方案中通常很难实现。
```



# 对接hive

```mysql
当Hive绑定了Hudi表时，查询并不是通过一个单独的"Hudi服务器"来执行的。实际上，Hudi与Hive的集成是通过Hudi的输入格式和Hive Metastore的同步来实现的。这里是一个简化的过程说明：
Hudi与Hive Metastore同步: 当你使用Hudi写入数据时，Hudi会负责将表的元数据信息同步到Hive Metastore。这包括表结构、分区信息以及数据文件的位置。
Hive查询执行: 当用户通过Hive执行查询时，Hive将根据查询计划去读取Hive Metastore中的元数据信息。如果查询涉及到Hudi管理的表，Hive会使用Hudi的输入格式来读取数据。
Hudi输入格式: Hudi提供了自己的输入格式，这些输入格式了解如何读取Hudi管理的数据文件（包括基础的Parquet文件和增量的日志文件）。当Hive执行器读取数据时，它实际上是通过Hudi的输入格式来访问存储在HDFS或兼容的存储系统上的数据。
处理增量读取: 对于支持增量读取的查询（即只读取自上次查询以来发生变化的数据），Hudi的输入格式将处理这种逻辑，确保只返回所需的数据集。
返回结果: 一旦Hive执行器通过Hudi的输入格式读取了数据，它将处理这些数据并返回查询结果给用户。
在整个过程中，并没有一个单独的"Hudi服务器"参与查询执行。Hudi的库与Hive紧密集成，使得Hive能够透明地查询Hudi表，而用户不需要关心数据是如何被Hudi管理的。这样的设计使得Hudi可以轻松地与现有的Hive查询工具和生态系统集成，而不需要额外的服务或组件。
```

# hudi模式

```mysql
Apache Hudi (Hadoop Upserts Deletes and Incrementals) 是一个为Apache Hadoop生态系统设计的数据存储层，它支持高效的实时写入操作。Hudi的主要特点是它支持对大型数据集进行快速的插入、更新和删除操作，这使得它非常适合需要高效写入和实时处理的用例，如流处理和增量数据管道。
Hudi支持两种类型的表：
Copy on Write (CoW): 在这种模式下，数据在写入时会创建一个新的数据文件，保持数据的不可变性。这适用于读多写少的场景，因为它优化了读性能，但写入会涉及重写文件。
Merge on Read (MoR): 这种模式下，写操作分为两个层面：一个是快速的追加到日志文件中，另一个是异步地将这些日志文件合并到列式存储文件中。这允许快速写入，同时为查询提供了近乎实时的数据视图。
关于与Hive的集成，Hudi提供了对Hive的支持，允许用户通过Hive查询Hudi数据集。Hudi可以同步数据到Hive Metastore，这样就可以使用Hive SQL直接对Hudi数据集进行查询。这包括支持分区，Hudi表的分区字段可以映射到Hive表的分区列，从而可以在Hive中进行高效的分区查询。
实现Hudi与Hive的集成，并保持高效写入的同时，可以通过以下几个方面来优化：
分区策略: 合理地设计分区策略可以减少单个分区的大小，提高查询效率。
索引: Hudi提供了索引功能来跟踪记录的位置，这有助于快速进行更新和删除操作。
文件大小: 控制文件大小可以减少文件打开和读取的开销。
写入模式: 选择合适的写入模式（CoW或MoR）可以根据应用场景优化性能。
资源配置: 为Hudi作业合理分配计算和存储资源。
并行度: 调整Hudi作业的并行度可以提高写入性能。
调优: Hudi提供了多种配置参数，可以调整来优化写入性能。
总的来说，Hudi是为高效实时写入而设计的，同时支持与Hive的集成，包括分区功能。通过合理配置和优化，可以在使用Hive时实现高效的写入和查询操作
```

