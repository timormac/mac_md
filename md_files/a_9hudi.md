# Hudi简介

```
Hive 和 Hudi 是两个不同的项目，它们都是在大数据生态系统中使用的，但它们的目的和功能有所不同。
Hive 是一个建立在 Hadoop 上的数据仓库软件，用于数据摘要、查询和分析。Hive 使得那些熟悉 SQL 的人可以通过 HiveQL（一种类似于 SQL 的查询语言）编写查询。这些查询然后被转换成一系列的 MapReduce、Tez 或 Spark 作业来执行。Hive 旨在提供一个 SQL 接近的接口来查询存储在 Hadoop 文件系统（HDFS）或其他兼容存储系统中的大数据集。
Hudi（Apache Hudi）代表 "Hadoop Upserts Deletes and Incrementals"，是一个为 Hadoop 生态系统构建的数据存储层，用于支持高效的插入、更新和删除操作。Hudi 通过提供快速的增量数据处理和数据变更流功能，使得在大数据平台上进行更复杂的数据管道操作成为可能。
Hudi 的主要特点和优势包括：
支持近实时的数据插入、更新和删除：Hudi 允许对存储的数据集进行快速修改，这对于需要频繁更新数据的应用程序非常重要。
增量处理：Hudi 支持增量读取，这意味着可以仅查询自上次查询以来发生变化的数据，从而提高查询效率。
时间旅行：Hudi 提供了查看数据在任意时间点的能力，这是通过跟踪数据的变化历史来实现的。
事务支持：Hudi 提供了对多个写操作的原子性保证，这对确保数据的一致性至关重要。
最初，Hudi 被设计出来是为了解决 Uber 大数据平台上数据存储的问题，特别是在高吞吐量的数据流中快速更新大型数据集的需求。Uber 需要一种方式来有效地处理大量的数据插入、更新和删除操作，同时保持数据的一致性和查询效率。Hudi 就是为了解决这些挑战而创建的。
与 Hive 相比，Hudi 提供了更为高级的数据管理功能，特别是对于需要处理数据变更和更新的场景。而 Hive 更多地被用于静态数据的批量处理和分析。两者可以结合使用，在 Hudi 管理的数据上运行 Hive 查询，以充分利用 Hive 的 SQL 查询能力和 Hudi 的高效数据管理。
```



# 对比hive+hadoop

```mysql
Apache Hudi (short for Hadoop Upserts Deletes and Incrementals) 是一个开源的数据管理框架，用于在Hadoop兼容的存储上管理流式和批量数据。Hudi 引入了几个关键的数据管理功能，这些功能对于构建高效、实时的数据湖和数据仓库至关重要。

与传统的 Hive+Hadoop 数据仓库相比，Hudi 提供了以下几个关键特性：

1. **支持Upserts和Deletes**：
   - **Upserts（更新和插入）**：Hudi 支持对存储在HDFS上的数据进行原地更新和插入操作，这是传统的Hive数据仓库不支持的。在Hive中，更新通常需要重新写整个分区或表。
   - **Deletes**：Hudi 也支持记录级别的删除，这在Hive中通常很难实现或需要昂贵的全表扫描。

2. **增量处理**：
   - Hudi 支持增量数据处理，这意味着可以只处理自上次查询以来新增或变更的数据，而不是重复处理整个数据集。这对于构建近实时的数据管道和提高处理效率至关重要。

3. **时间旅行（Time Travel）**：
   - Hudi 提供了数据的时间旅行功能，允许用户查询历史数据版本，这对于数据回溯、审计和修复错误数据非常有用。

4. **更快的查询性能**：
   - 通过使用索引和增量拉取，Hudi 可以提供比传统的Hive查询更快的查询性能，尤其是在处理大量数据时。

5. **事务支持**：
   - Hudi 提供了ACID事务，这保证了即使在高并发的写入场景下，数据也能保持一致性。

6. **易于集成**：
   - Hudi 可以与现有的Hadoop生态系统工具（如 Hive、Spark 和 Presto）无缝集成，使得存储在Hudi数据集中的数据可以被这些工具查询。

在以下场景下，你可能需要考虑将现有的 Hive+Hadoop 数据仓库迁移到 Hudi：

- 当你需要支持高频率的数据更新和删除操作时。
- 当你需要减少数据处理的延迟，并希望实现近实时的数据同步或流处理时。
- 当你需要对数据进行审计、错误修复或历史数据分析时。
- 当你希望提高大数据集上的查询性能时。
- 当你的数据管道需要处理大量的增量数据，并希望提高效率和减少资源消耗时。

总的来说，如果你的数据处理工作流需要高效的数据更新、删除、增量处理和事务支持，那么迁移到 Hudi 可能是一个值得考虑的选择。
```



# 对接hive

```mysql
当Hive绑定了Hudi表时，查询并不是通过一个单独的"Hudi服务器"来执行的。实际上，Hudi与Hive的集成是通过Hudi的输入格式和Hive Metastore的同步来实现的。这里是一个简化的过程说明：
Hudi与Hive Metastore同步: 当你使用Hudi写入数据时，Hudi会负责将表的元数据信息同步到Hive Metastore。这包括表结构、分区信息以及数据文件的位置。
Hive查询执行: 当用户通过Hive执行查询时，Hive将根据查询计划去读取Hive Metastore中的元数据信息。如果查询涉及到Hudi管理的表，Hive会使用Hudi的输入格式来读取数据。
Hudi输入格式: Hudi提供了自己的输入格式，这些输入格式了解如何读取Hudi管理的数据文件（包括基础的Parquet文件和增量的日志文件）。当Hive执行器读取数据时，它实际上是通过Hudi的输入格式来访问存储在HDFS或兼容的存储系统上的数据。
处理增量读取: 对于支持增量读取的查询（即只读取自上次查询以来发生变化的数据），Hudi的输入格式将处理这种逻辑，确保只返回所需的数据集。
返回结果: 一旦Hive执行器通过Hudi的输入格式读取了数据，它将处理这些数据并返回查询结果给用户。
在整个过程中，并没有一个单独的"Hudi服务器"参与查询执行。Hudi的库与Hive紧密集成，使得Hive能够透明地查询Hudi表，而用户不需要关心数据是如何被Hudi管理的。这样的设计使得Hudi可以轻松地与现有的Hive查询工具和生态系统集成，而不需要额外的服务或组件。
```

# hudi模式

```mysql
Apache Hudi (Hadoop Upserts Deletes and Incrementals) 是一个为Apache Hadoop生态系统设计的数据存储层，它支持高效的实时写入操作。Hudi的主要特点是它支持对大型数据集进行快速的插入、更新和删除操作，这使得它非常适合需要高效写入和实时处理的用例，如流处理和增量数据管道。
Hudi支持两种类型的表：
Copy on Write (CoW): 在这种模式下，数据在写入时会创建一个新的数据文件，保持数据的不可变性。这适用于读多写少的场景，因为它优化了读性能，但写入会涉及重写文件。
Merge on Read (MoR): 这种模式下，写操作分为两个层面：一个是快速的追加到日志文件中，另一个是异步地将这些日志文件合并到列式存储文件中。这允许快速写入，同时为查询提供了近乎实时的数据视图。
关于与Hive的集成，Hudi提供了对Hive的支持，允许用户通过Hive查询Hudi数据集。Hudi可以同步数据到Hive Metastore，这样就可以使用Hive SQL直接对Hudi数据集进行查询。这包括支持分区，Hudi表的分区字段可以映射到Hive表的分区列，从而可以在Hive中进行高效的分区查询。
实现Hudi与Hive的集成，并保持高效写入的同时，可以通过以下几个方面来优化：
分区策略: 合理地设计分区策略可以减少单个分区的大小，提高查询效率。
索引: Hudi提供了索引功能来跟踪记录的位置，这有助于快速进行更新和删除操作。
文件大小: 控制文件大小可以减少文件打开和读取的开销。
写入模式: 选择合适的写入模式（CoW或MoR）可以根据应用场景优化性能。
资源配置: 为Hudi作业合理分配计算和存储资源。
并行度: 调整Hudi作业的并行度可以提高写入性能。
调优: Hudi提供了多种配置参数，可以调整来优化写入性能。
总的来说，Hudi是为高效实时写入而设计的，同时支持与Hive的集成，包括分区功能。通过合理配置和优化，可以在使用Hive时实现高效的写入和查询操作
```

