# flink实时项目

```mysql
#案例
订单,关联商品表,关联用户表,关联商户表。
统计当天订单数,订单金额,用户数,top商品售卖数,购买渠道(app，小程序,公众号推文)。
各小时订单数,订单金额,用户数,top商品售卖数,购买渠道(app，小程序,公众号推文)


#statebackend
（细节需要写项目发现）
哪些状态用rockdb
哪些状态用hashmap:各小时商品售卖数

#ttl
统计各小时商品售卖数,按dt|spu作为key,然后设置ttl为1小时

#checkpoint故障恢复数据正确
kafka偏移量提交是通过 checkpoint 统一提交的。当偏移量还原时，累加值操作也会还原，没有问题。
如果不是checkpoint统一提交，手动提交，那么累加值无法保证。
累加值还原了，不过重复消费问题没解决,这个需要下游幂等性来做？具体待补充

#flink精准一次性消费kafka

#如何判断资源是否够用
通过flink的Web界面中,您可以查看任务的运行状态、度量指标和资源使用情况。通过查看任务的CPU使用率、内存使用率和网络IO等指标。以及高峰期间使用率，是否反压,来判断资源是否足够用。


```

# 离线在线混搭







# spark实时项目





# hive八股文

## 数据倾斜

```mysql
#出现场景
主要是热点问题,某个key占绝大部分数据,导致groupby或者join倾斜。
热点问题，第一个就是空值/默认值，第二个就是热点

#解决方案1 mapjoin
大表join小表
map join,默认有个20m配置,如果集群内存够大可以修改这个参数。

#方案2 热点key打散
1 空值，默认值情景打散，if判断如果key是"",则加随机数,若非空则正常，关联维度表
2 热点key打散,在groupby场景下,给key+｜随机数(1-10)先聚合一次，然后再按｜切分再聚合一次
3 热点key打散,在join场景下,给key+｜随机数(1-10),然后维度表去笛卡尔积1-10扩大10倍。这个场景是维度表数量级远小于主表,但是维度表也相对大，不能mapjoin加载到内存



#方案3 热点单独处理
大表join中维度表
1 先从大表 执行group by shopid having count(*)>100000,来找出热点商铺。因为groupby 可以执行map端预聚合，所以shuffle时,数据量小,不会出现group by这里出现热点问题导致速度慢，然后存入临时表。

2 用大表 join 热点商户表,过滤出热点商户数据,因为热点商户表数据很小，这里是mapjoin没有大批量shuffle。
然后中维度表关联热点表,过滤出热点商户维度，再关联上面过滤的，中维度表过滤后，也是小表是map join 没有shuffle

3 总结为什么会快
步骤1执行后的数据,步骤2执行后的数据都是临时mr数据，因为都是mapjoin，所以reduce结果都在本节点落盘。
当在join时，把维度表作为mapjoin，这样热点数据是多服务器计算，不是单服务器，并且没有跨节点的shuffle落盘，只有本地落盘

4 再union all非热点数据，和流程2一样，区别就是非热点的商户是个大表，不能mapjoin。这里就是热点的mapjoin+非热点的shuffle


```



# flink八股文

## 任务提交模式

```mysql
#session
session和application都是yarn模式，
session只有一个集群,适合长久运行和频繁提交作业,session模式下,JobManager 和 TaskManager是分开的,提交的多个flink任务共用一个JobManager.
session只会启动一个flink集群,

#application
run-application模式下，JobManager 和 TaskManager 是一起的。JobManager 和 TaskManager 运行在同一进程中，共享相同的资源，这种模式适用于单机或小规模的部署。
但是session模式下,JobManager 和 TaskManager是分开的，所有任务共享一个JobManager

#per-job
per-job和run-application，每个任务都会单独启动集群，不过run-application和yarn对接更好，如历史服务器等。

#对比
session模式能减少创建jobmanager的开销,application最好是某个单独任务需要独立的，怕被影响的，或者独自内存需求很大的
```

## 架构模型

```mysql
#jobmanager

#taskmanager
是一个jvm进程，每个taskmanager，都需要启动个yarn的container
slot时taskmanager的线程，taskmanager的jvm可以有多个线程slot,

#slot
虽然多个slot是一个taskmanager的jvm下，但是slot不共享内存，2个算子还是要序列化和反序列化。
可以理解成每个slot是单独的程序，只是在同一个jvm下运行。

3个map算子到key by 到1个keyed算子，一共4个任务:3+1    3个slot就可以执行
一个slot可以执行多个算子，既可以在map阶段算子，也可以处理reduce阶段的算子。
如果某个算子工作量大，可以不设置slot共享，这样那个算子会单独占用一个slot。

案例1
yarn-session模式下，启动时设置4个taskmanager,每个4个插槽，共16个slot。你占用15个，剩下一个还是可以提交flink任务的。
因此同一个jvm,不同slot可以执行不同的flink任务，所以slot之间不互通

案例2
yarn-session模式下，启动时设置4个taskmanager,每个4个插槽，共16个slot。你占用15个，剩下一个还是可以提交flink任务的。
因此同一个jvm,不同slot可以执行不同的flink任务，所以slot之间不互通
一个jvm可以执行多个不同的java任务。java a.class指令，每次执行会单独创建一个jvm。可以通过工具提交java任务，到之前的jvm。
这个就是以前不理解的jvm重用，现在理解了。


#注意
虽然多个slot是一个taskmanager的jvm下,虽然多个slot共享内存，2个算子还是要序列化和反序列化。
可以理解成每个slot是单独的程序，只是在同一个jvm下运行。
个人理解是flink拆解任务时，不能根据你slot的上下游是否在一个taskmanager中，来优化不用序列化
因为有的是不在一个taskmanager中的，必须序列化，为了统一只能统一都序列化


```

## 反压机制

## 编码概念

#### state

#### ttl

#### checkpoint

#### statebackend

#### watermark









# spark八股文