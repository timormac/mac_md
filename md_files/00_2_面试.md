# 项目概览

### flink实时项目

```mysql
#案例
订单,关联商品表,关联用户表,关联商户表。
统计当天订单数,订单金额,用户数,top商品售卖数,购买渠道(app，小程序,公众号推文)。
各小时订单数,订单金额,用户数,top商品售卖数,购买渠道(app，小程序,公众号推文)


#statebackend
（细节需要写项目发现）
哪些状态用rockdb: 
		大公司比如实时热门商品，商品很多的情况下需要。
		每个图标点击次数，点击的人数,
		统计当天购买人数(去冲)，多次下单
哪些状态用hashmap:各小时商品售卖数

#ttl
统计各小时商品售卖数,按dt|spu作为key,然后设置ttl为1小时

#checkpoint故障恢复数据正确
设置checkpoint，应根据ttl时间，确定是用changelog增量变化，还是状态整体同步。

kafka偏移量提交是通过 checkpoint 统一提交的。当偏移量还原时，累加值操作也会还原，没有问题。
如果不是checkpoint统一提交，手动提交，那么累加值无法保证。
累加值还原了，不过重复消费问题没解决,这个需要下游幂等性来做？具体待补充

#flink精准一次性消费kafka

#如何判断资源是否够用
通过flink的Web界面中,您可以查看任务的运行状态、度量指标和资源使用情况。通过查看任务的CPU使用率、内存使用率和网络IO等指标。以及高峰期间使用率，是否反压,来判断资源是否足够用。

#数据流join（问题待解决）
flink是流处理，可以2个流connect得到conn流。因为多并行度需要keyby会对key取hash进行分配并行度。
然后再process里定义2个map进行存储，流数据进入，进行关联

```

### 离线在线混搭

```mysql
#表任务筛选
ods层新增和变化的表大表,这种表在hive中,动态分区插入,join关联历史表效果不好。
主业务表：订单,核销，预约表。

#框架选型
实时任务,要能支持实时高效写入.
筛选的表，有变更数据,要能支持数据变更
flink故障恢复问题，要能支持重复数据解决.

hbase 支持数据变更，幂等性,不过绑定hql表查询任务是走服务器,性能需要优化
hudi 支持数据变更,对于hive更友好，执行hql走mr程序。

#解决数据更新,重复数据问题
重复问题是chekpoint故障恢复时会有，kafka是由ck来提交的
更新直接写入hbase

#hbase表rowkey的合理设计，以及查询性能问题
hbase表不支持hive分区,需求过滤当天订单，2个方案
方案1: rowkey是日期+订单号,利用hbase前缀过滤,不过这个会出现热点问题,历史订单表都在一个region,性能不好
方案2: 设置日期二级索引,如日期+订单号,虽然会集中在

#任务变更
原来的都是过滤分区，现在把日志索引作为那个字段


```



### 数仓质量管理

```mysql
#数仓中的链路表进行条数，度量进行校验
出现条数不一致原因,模型变动覆盖不到。维度表多条,后端只取一条
每日脚本,hive中数据，条数，总金额数，写入hbase，然后人工配置一些校验。条数不为0,条数相等，总额度相同。
还有些通过指标推指标的,不过逻辑复杂。


#通过血缘关系找到重复中间表进行合并，制定临时表建表规范(好复杂)
元数据表，记录表的血缘表,统计口径(group by ,full,filter)
建表时输入到的关联关系表,查询是有已有临时表，如果字段没覆盖到,字段补充


#找出耗时较多，并且可以优化的任务，执行优化
dwt 2套脚本维护,无法重跑。金额统计不对,就是珍珠蚌，不应该加入

#历史任务监控，停用

```





### spark实时项目

```mysql

```





### 数仓模型设计

```mysql
#ods
业务数据每日导入方式,订单 新增及变化。分区增量
#dim
小维度表,分区，每日全量，保持历史状态
大维度表 用户表新增及变化(如果是拉链表用什么分区呢？)
#dwd
和ods差不多,区别就是和dim关联,放入维度相关其他主键,做数据清洗（日期格式转换，空值处理）。
脏数据原因，后端没落部分字段(标题,部分字段)。

#dws
#dwt
```



# 技术栈

### flink

```mysql
#内存模型


#参数调优

#资源合理分配

#状态编程

#任务监控

#反压机制

#精准一次性消费

#flink-core核型原理


#flink-sql
```



### hive

```mysql
#参数调优
开启 map join,根据集群内存规模yarn的container大小,hdfs的块大小,设置合理的小表大小
开启map端预聚合,合理设置抽样,根据块大小,数据量条数，设置超过多少条开机预聚合
开启skew join,设置数据量超过多少，进行热点join处理


#执行计划
from的表，不需要select需要字段，hive优化器会根据你用到的字段，自动过滤。直接from a就行



#数据倾斜问题
1 热点key group场景,大部分预聚合。不能预聚合的加随机数聚合一次,然后去随机数再聚合一次
2 空值问题，空值打散，if判断如果key是"",则加随机数,若非空则正常，关联维度表会自动过滤掉空值
3 热点key join场景下,给key+｜随机数(1-10),然后维度表去笛卡尔积1-10扩大10倍。这个场景是维度表数量级远小于主表,但是维度表也相对	大，不能mapjoin加载到内存
4 热点key join场景下,热点单独处理：
   	先从大表 执行group by shopid having count(*)>100000,来找出热点商铺。因为groupby 可以执行map端预聚合，所以shuffle			时,数据量小,不会出现group by这里出现热点问题导致速度慢，然后存入临时表。

  	用大表 join 热点商户表,过滤出热点商户数据,因为热点商户表数据很小，这里是mapjoin没有大批量shuffle。
		然后中维度表关联热点表,过滤出热点商户维度，再关联上面过滤的，中维度表过滤后，也是小表是map join 没有shuffle

  	总结为什么会快
		步骤1执行后的数据,步骤2执行后的数据都是临时mr数据，因为都是mapjoin，所以reduce结果都在本节点落盘。
		当在join时，把维度表作为mapjoin，这样热点数据是多服务器计算，不是单服务器，并且没有跨节点的shuffle落盘，只有本地落盘



#hive优化--------
# 拉链表  
数据量大的维度表，每日变化不多

#合理设置map,reduce数
减少map任务数：在map执行前合并小文件，减少map数：CombineHiveInputFormat具有对小文件进行合并的功能（系统默认的格式）
reduce数：用hive自己的算法,默认:每个Reduce处理的数据量默认是256MB


#减少小文件数
1  在map-only任务结束时合并小文件，默认true 。SET hive.merge.mapfiles = true;
2  在map-reduce任务结束时合并小文件，默认false  SET hive.merge.mapredfiles = true;
	 合并文件的大小，默认256M  SET hive.merge.size.per.task = 268435456;
	 当输出文件的平均大小小于该值时，启动一个独立的map-reduce任务进行文件merge，SET hive.merge.smallfiles.avgsize = 		    16777216;



```



### spark

### kafka

### hbase

### hadoop





# hive八股文

### 数据倾斜

```mysql
#出现场景
主要是热点问题,某个key占绝大部分数据,导致groupby或者join倾斜。
热点问题，第一个就是空值/默认值，第二个就是热点

#解决方案1 mapjoin
大表join小表
map join,默认有个20m配置,如果集群内存够大可以修改这个参数。

#方案2 热点key打散
1 空值，默认值情景打散，if判断如果key是"",则加随机数,若非空则正常，关联维度表
2 热点key打散,在groupby场景下,给key+｜随机数(1-10)先聚合一次，然后再按｜切分再聚合一次
3 热点key打散,在join场景下,给key+｜随机数(1-10),然后维度表去笛卡尔积1-10扩大10倍。这个场景是维度表数量级远小于主表,但是维度表也相对大，不能mapjoin加载到内存



#方案3 热点单独处理
大表join中维度表
1 先从大表 执行group by shopid having count(*)>100000,来找出热点商铺。因为groupby 可以执行map端预聚合，所以shuffle时,数据量小,不会出现group by这里出现热点问题导致速度慢，然后存入临时表。

2 用大表 join 热点商户表,过滤出热点商户数据,因为热点商户表数据很小，这里是mapjoin没有大批量shuffle。
然后中维度表关联热点表,过滤出热点商户维度，再关联上面过滤的，中维度表过滤后，也是小表是map join 没有shuffle

3 总结为什么会快
步骤1执行后的数据,步骤2执行后的数据都是临时mr数据，因为都是mapjoin，所以reduce结果都在本节点落盘。
当在join时，把维度表作为mapjoin，这样热点数据是多服务器计算，不是单服务器，并且没有跨节点的shuffle落盘，只有本地落盘

4 再union all非热点数据，和流程2一样，区别就是非热点的商户是个大表，不能mapjoin。这里就是热点的mapjoin+非热点的shuffle


```



# flink八股文

### 任务提交模式

```mysql
#session
session和application都是yarn模式，
session只有一个集群,适合长久运行和频繁提交作业,session模式下,JobManager 和 TaskManager是分开的,提交的多个flink任务共用一个JobManager.
session只会启动一个flink集群,

#application
run-application模式下，JobManager 和 TaskManager 是一起的。JobManager 和 TaskManager 运行在同一进程中，共享相同的资源，这种模式适用于单机或小规模的部署。
但是session模式下,JobManager 和 TaskManager是分开的，所有任务共享一个JobManager

#per-job
per-job和run-application，每个任务都会单独启动集群，不过run-application和yarn对接更好，如历史服务器等。

#对比
session模式能减少创建jobmanager的开销,application最好是某个单独任务需要独立的，怕被影响的，或者独自内存需求很大的
```

### 架构模型

```mysql
#jobmanager

#taskmanager
是一个jvm进程，每个taskmanager，都需要启动个yarn的container
slot时taskmanager的线程，taskmanager的jvm可以有多个线程slot,

#slot
虽然多个slot是一个taskmanager的jvm下，但是slot不共享内存，2个算子还是要序列化和反序列化。
可以理解成每个slot是单独的程序，只是在同一个jvm下运行。

3个map算子到key by 到1个keyed算子，一共4个任务:3+1    3个slot就可以执行
一个slot可以执行多个算子，既可以在map阶段算子，也可以处理reduce阶段的算子。
如果某个算子工作量大，可以不设置slot共享，这样那个算子会单独占用一个slot。

案例1
yarn-session模式下，启动时设置4个taskmanager,每个4个插槽，共16个slot。你占用15个，剩下一个还是可以提交flink任务的。
因此同一个jvm,不同slot可以执行不同的flink任务，所以slot之间不互通

案例2
yarn-session模式下，启动时设置4个taskmanager,每个4个插槽，共16个slot。你占用15个，剩下一个还是可以提交flink任务的。
因此同一个jvm,不同slot可以执行不同的flink任务，所以slot之间不互通
一个jvm可以执行多个不同的java任务。java a.class指令，每次执行会单独创建一个jvm。可以通过工具提交java任务，到之前的jvm。
这个就是以前不理解的jvm重用，现在理解了。


#注意
虽然多个slot是一个taskmanager的jvm下,虽然多个slot共享内存，2个算子还是要序列化和反序列化。
可以理解成每个slot是单独的程序，只是在同一个jvm下运行。
个人理解是flink拆解任务时，不能根据你slot的上下游是否在一个taskmanager中，来优化不用序列化
因为有的是不在一个taskmanager中的，必须序列化，为了统一只能统一都序列化


```

### 反压机制

### 编码概念

#### state

#### ttl

#### checkpoint

#### statebackend

#### watermark









# spark八股文