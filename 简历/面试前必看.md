# 面试说什么

4 企业中怎么发现数仓数据不对的，怎么处理的

```sql
#数仓模型
开始有dwt层,后面发现数据有问题,并且不支持重跑(因为是insert into),后面这种不支持重跑的，全都删了。
人工修改数据很麻烦,要么写一个修复任务，全量跑到今天。要么就是把这个任务执行365次，太慢。

#dwt问题原因
1 本身数据有问题；上游dwd,dws数据发现问题，需要改逻辑重跑；洗数据等；造成dwt数据污染,无法恢复
2 dwt多并行任务其中出错，insert into导致数据污染，这个要开启事务才能解决，insert into 的共同毛病

#离在线混搭问题
问题详情看下面


```







# 实时推荐项目

改架构模型:  前端埋点，原本是批发送，或者用户下线时批量发送，点一下发一下。后端下订单直接发送到kafka消息队列。沟通





# 数据仓库离在线混搭

### aqyi离在线混搭

```sql
他们的要求和我们的不一样,他们不是把离线作业更改为实时，白天实时跑。

他们是把离线任务服务器和实时任务服务器，放在一起，提高cpu利用率，节省资源。
因为他们有的事小时级别的任务,并不是只有天级别的。所以白天也用。
他们解决的问题还有，当实时服务器的高峰，中午和晚上来临时，还要杀死离线任务，然后在空闲时间重跑等复杂问题。
```

### 工作流程

```mysql
#找到可转换为实时的任务
1 数据同步任务
2 小时级别的任务
3	ods层好像都可以(但是只做主要大表)
4 dwd能不能做，后续待确定

#故障恢复/数据安全问题
离线任务直接重跑就行（如果是overwrite的表，insert的表也不行）
实时要保证故障恢复后，数据准确checkpoint，事务，变更表的数据，性能





```



### 面试说法

```sql
#方案1 
第一次选用flink写到hdfs上，虽然fileSink实现了两阶段提交，但是只能保证检查点成功触发时内部的事务，如果检查点失败，会重复数据。
而且flink也无法对hdfs的数据进行修改删减。后面想到通过ACID表来做数据修改，不过只支持hive服务端走mr，不支持直接修改hdfs对应的
ACID表写入来修改数据。后面这个方案pass了。

#方案2 
选hbase原因,支持修改数据,故障恢复 checkpoint写重复数据也没问题
但是要考虑并行度,hbase大范围查询的性能, 表预分区和rowkey设计好

后面可以说，想到了一个坑,就是hql多个任务同时调用查hbase的regionServer时，不知道能不能扛住。
需要看explain执行计划。
之前数据量小，并且修改的hbase表不多，所以没问题。
最好还是用hudi，用文件支持数据访问，对hive更友好。

#看了一些大公司资料方案
离在线混搭就是看爱奇艺的，但是有的他们的问题我们不需要做，比如离线为高峰让路等。

#优化措施(不过dataworks不支持这么做)
flink写hive的ods层表数据,之后要hive去load才能加载到数据。不过对于hbase表，不用load,写入时是通过服务端
hive load时把数据合并，输出成一个文件。



```



### 需解决问题

##### 离线改实时注意点

```sql
#简介

#离线的问题
离线改实时，需要注意：离线任务通过maxwell,sqoop可以通过框架本身的事务，保证数据精准写入，比如sqoop设置mr任务为1，保证不会出现部分数据成功的问题。并且失败后，直接重跑就行。
离线任务有数据漂移，而实时任务不会有数据漂移，就是在sqoop导入的过程中，where条件是新增或变化为当天。那个时间点有数据改变了，解决方法就是晚几分钟执行。


#实时问题
第一保证任务挂了后，任务恢复后如何数据还是精准，具体看下方:大概就是checkpoint+幂等性hbase。

#资源利用问题
实时任务的资源利用，如何保证不会浪费资源，因为实时任务是永远再跑的




```



##### 资源浪费问题

```sql
#问题
数据仓库中，想把一些数据离线任务改为在线flink任务实时处理。现在的问题就是，如果每有一个新的实时任务，那么就要新提交一个flink任务。但是flink任务每次至少启动一个jobmanager和taskmanager而申请yarn系统的container时，每个至少1个G的资源。但是我这个实时任务不需要那么大，并且我也不可能把这个实时任务的代码合并到别的实时任务代码里，那么有什么解决方案呢

#解决方案
1. **Flink Session 集群**:
   使用 Flink Session 集群可以共享 JobManager 和 TaskManagers，这样多个任务可以在同一个集群上运行，而不需要为每个任务启动新的集群。你可以提交多个作业到同一个 Session 集群，并且它们将共享集群资源。

2. **任务合并**:
   考虑将功能相似或数据流类似的任务合并为单个 Flink 任务，这样可以在一个任务中处理多个数据流，从而减少资源需求。

3. **Flink on Kubernetes**:
   如果你的环境支持 Kubernetes，可以考虑在 Kubernetes 上运行 Flink。Kubernetes 有更好的资源管理和调度能力，可以更高效地利用资源。Flink Kubernetes Operator 可以帮助管理 Flink 应用的生命周期。

4. **Flink 应用模式**:
   在 Flink 1.11 及以后的版本中，引入了 Application Mode，这种模式下，Flink 应用和 JobManager 运行在同一个进程中，这样可以减少资源的开销。
```







##### 数据一致性/变动数据

flink写hive要解决小文件问题。

最开始hive不支持更新操作，类似订单表只能当天最后状态。如果想用实时来做，无法完成对应分区更细修改数据。

因为hdfs只支持增量写入，不支持修改。但是hive新版有了ACID后，对于修改的数据，可以支持了。

现在又遇到的问题就是,flink不支持ACID写入。flink是针对hive表的元数据格式，直接对hdfs文件做处理的。

这样才能保证快，因为不是通过hive写入，所以没法完成hive的ACID。通过hive就算写一条数据，也要走mr这种效率太低了。

所以现在问题卡在这里了。

如果把hive表用hbase来绑定，也有个问题，就是当大量job任务来来查询是，hbase能不能向mr一样，同时支持并快速的响应数据支持。因为hive绑定hbase是通过hbase服务器来响应数据的，而不像hive直接大家拿hdfs文件就行。



如果想最佳的解决这个问题,那么了解hiveACID的原理，通过flink手动维护ACID文件来完成修改数据。

简单的解决思路，flink对修改的数据进行缓存,定期通过hive客户端插入到hive表中，走mr。

查询了些资料：hudi在这方面挺合适的，也能和hive对接，和habase有点像，不过没搞，用其他方案解决的。

hudi介绍

```
Apache Hudi 和 Apache HBase 都是大数据生态系统中的重要组件，但它们是为了不同的用例和场景设计的。以下是它们之间的一些关键区别，以及在何种场景下可能会选择 Hudi 而不是 HBase：

### Apache Hudi

- **用例**：Hudi 主要用于构建和管理数据湖，它支持在大规模数据集上进行高效的插入、更新和删除操作。Hudi 适用于需要处理批量和流数据的场景，特别是在数据仓库和数据湖架构中。
- **存储模型**：Hudi 存储在HDFS或兼容的文件系统（如Amazon S3）上，使用文件作为数据存储的基本单位，并且支持列式存储格式，如Parquet和ORC，这对于分析查询是优化的。
- **一致性和事务**：Hudi 提供了快照隔离和事务支持，使得在大数据环境中进行复杂的数据管理成为可能。
- **实时性**：Hudi 支持近实时的数据处理和查询，特别适合需要频繁更新数据集的场景，如数据湖实时同步、增量ETL管道和流处理。

### Apache HBase

- **用例**：HBase 是一个分布式、可扩展的NoSQL数据库，设计用于处理随机、实时的读/写访问大型数据集。HBase 适用于需要低延迟访问的在线服务，如用户界面后端、实时分析和监控系统。
- **存储模型**：HBase 在 HDFS 上以键值对的形式存储数据，是一个面向列的数据库，适合处理大量的非结构化或半结构化数据。
- **一致性和事务**：HBase 提供了强一致性的读写能力，但不支持跨多行的事务。
- **实时性**：HBase 优化了低延迟的数据访问和更新，适合需要快速响应的应用程序。

### 场景对比

以下是选择 Hudi 而不是 HBase 的一些具体场景：

1. **数据湖同步**：如果你正在构建一个数据湖，并且需要处理来自不同数据源的数据，同时又需要支持数据的插入、更新和删除操作，Hudi 是一个更好的选择。Hudi 可以轻松地与现有的数据仓库解决方案（如Hive）集成，支持复杂的ETL作业和增量数据处理。

2. **分析工作负载**：对于需要高效进行批量分析查询的场景，Hudi 提供了基于列的存储格式（如Parquet），这对于分析型查询是优化的。HBase 的行存储模型并不适合批量分析工作负载。

3. **变更数据捕获（CDC）**：如果你需要实现变更数据捕获，以便将数据库的变更实时同步到数据湖中，Hudi 提供了更好的工具和机制来处理这种类型的工作流。

4. **增量数据处理**：在需要增量读取和处理数据的场景下，Hudi 提供了增量查询的能力，这使得只处理自上次查询以来发生变化的数据成为可能，从而提高效率。

例如，如果一个电子商务公司想要实时分析用户的交易数据，并且这些数据存储在数据湖中，同时需要能够处理订单更新或取消，那么使用 Hudi 将是更合适的选择。Hudi 可以确保数据的实时更新，并且可以直接集成到现有的分析工具和查询引擎中，如Hive和Spark。而如果这个公司需要一个用户行为的实时推荐系统，可能会选择 HBase 来存储和查询用户的点击流数据，因为它可以提供快速的随机访问性能。
```



使用hbase表代替hive

需要测试下，如果对hbase数据查询，做开窗等操作，是在hive中做，还是在hbase中做。

或者获取全量数据难道是通过hbase客户端一条条查询然后输出出去吗。。。

```
hive中A表绑定一个hbase外部表。当我有很多job任务需要用到A表时，这个job获取a表数据，是通过hbase的客户端查询数据？还是直接通过底层的hdfs文件拿到数据?效率会比直接在hive上创建一个A表来说，效率低吗
2023/12/20 12:30:09

在Hive中绑定一个HBase外部表时，你实际上是在创建一个Hive表的元数据，这个元数据指向了HBase中的表。这意味着，当你通过Hive查询这个外部表时，Hive将使用HBase的客户端API来与HBase交互，获取所需的数据。Hive将转换SQL查询为相应的HBase操作，比如get和scan。
因此，当你的job任务需要使用这个绑定了HBase的A表时，数据的获取是通过HBase客户端与HBase服务器通信来完成的，而不是直接从HDFS文件中读取。这种方式的效率与直接在Hive上创建一个内部表并直接从HDFS读取数据相比，通常会有所不同：
延迟: HBase是为了优化低延迟的随机读写而设计的，而Hive通常用于批量处理大量数据。如果你的查询是随机访问少量的行，HBase可能会提供更好的性能。但是，如果你要扫描大量的数据，直接通过Hive访问HDFS可能会更快，因为这样可以利用Hive的优化，比如列式存储格式（如Parquet或ORC）。
资源使用: 通过HBase客户端API访问数据意味着你的查询会通过HBase的RegionServers进行，这可能会影响HBase集群的性能，尤其是当有大量的并发查询时。而Hive直接访问HDFS则主要受到Hadoop集群资源的限制。
数据局部性: HBase优化了数据的局部性，这意味着相关数据通常存储在一起，这对于某些类型的查询非常有效。但是，Hive查询通常是针对大量数据的，数据局部性的优势可能就不那么明显了。
复杂查询: Hive支持的SQL功能比HBase强大，对于复杂的分析查询，Hive可能会更加适合。
在性能方面，没有一个统一的答案，它取决于你的工作负载特性和你的集群配置。在某些情况下，直接在Hive上创建内部表并从HDFS读取数据可能会更快，特别是对于大规模的批处理作业。对于需要频繁的小范围查找的作业，使用HBase可能会更有优势。
为了最佳性能，你可能需要根据你的具体使用情况进行基准测试，比较直接使用Hive内部表和通过HBase外部表访问数据的性能差异。
```





### 离在线混搭/实时仓库区别

​	个人理解，离线在线混搭，解决的问题，是大量任务在凌晨执行，对于数据查询的实时性没那么高，只是为了提高白天资源利用率，

​    用不到毫秒级别的响应。离在线混搭是为了减少一些当天数据的处理时间，对于历史数据和涉及到历史统计的，还是要用离线仓库去  	做，不可能用实时任务去完全替代。并且一些很复杂的hql，clickhouse没发处理，比如流失率等等，7天内连续登陆问题，复购率

 

​	而实时仓库，是为了毫秒级别的响应，查看app，当天运营状态，所以要引入clickhouse框架等。



### 设计理念

最开始hive不支持更新操作，类似订单表只能当天最后状态。如果想用实时来做，无法完成对应分区更细修改数据。

因为hdfs只支持增量写入，不支持修改。但是hive新版有了ACID后，对于修改的数据，可以支持了。

所以有了实时处理dwd数据的想法。 

现在又遇到的问题就是,flink不支持ACID写入。flink是针对hive表的元数据格式，直接对hdfs文件做处理的。

这样才能保证快，因为不是通过hive写入，所以没法完成hive的ACID。通过hive就算写一条数据，也要走mr这种效率太低了。

所以现在问题卡在这里了。







### 方案1:flink写hive

flink直接写入hive表上，这个夭折了，开发难度大，自己维护幂等性难，故障恢复处理难，hive中acid事务表无法处理。

##### 方案1缺点

```
#哪些层哪些表能用flink代替
ods可以，dwd待定，待思考。

用分区方法遇到的问题，有的表没有分区，改成分区表，则相关依赖的sql都用改。
所以对于没分区的表，继续用原来的表，这种表通常不台大。

flink没法读写acid表，自己要维护事务插入，也没法对接hive的acid因为不知道实现原理。
代码开发难度太大，注意问题太多，重复数据处理，故障数据恢复等。
只对几个主表进行这么操作，主业务表，其他支线都根据主表来跑，其他的是能并行的。


#凌晨读取数据，锁表问题
肯能导致数据写入不进去，需要去试。
解决方法：
1 分区，这里有个问题，要更改原hive任务的hql
2 hive的ACID表

用分区方法遇到的问题，有的表没有分区，改成分区表，则相关依赖的sql都用改。
所以对于没分区的表，继续用原来的

#要想实现dwd层，数据更新问题
hive新版本的ACID表，现在读取性能更好，用ACID表来做。
flink不支持读取和写入acid表

#kafka数据的精准一次性消费问题
用flink写入kafka一次性消费，详情看flink.md。代码实现复杂，并且影响吞吐量。
最简洁的方法是用table_api来写,然后出现问题了，手动去分区重跑数据。
tableAPI好像也能实现精准一次消费。


#任务故障，怎么数据恢复
正常情况重启就行，kafka数据继续消费。
如果真的出现数据条数不同，数据多了或者丢了，用原来的离线脚本执行问题分区。

#dwd改写，开发难度大，耗时高
本来hive就是为了加快速度的，减少mr编写。
如果实现了dwd改写，那也很复杂，想象怎么能提高dwd改写速度。
目前的思路是每小时执行一次，然后12点合并，但是怕有的表会改动数据，或者关联不到的情况。待处理

#什么时候接入系统的
把以前离线仓库的脚本任务的主任务，涉及到的表用flink任务重新编写，每天对比数据，数据稳定之后，替换表数据来源。

#怎么分配内存使用的
压侧，顶峰等具体看flink优化视频里有

```





### 方案2:写hbase

 flink写入hbase表，通过hive对接hbase表。但是hive表联通hbase表后，就无法使用 分区功能了，所以为了能够快速定位当日数据，要为hbase添加dt字段索引，并且这样也不会改变下一层的用到这张表的任务。

用hbase还有些优化的地方，比如hive使用hbase表的原理。hive用hbase表的话，执行mr任务，还能多平行度吗？如何多并行度多，还能手动指定map数吗等等。



##### 方案2实现

```
我在hive上如何对接hbase中的表，当我在hbase上更新数据时,hive表会增加数据吗？当我在hive表导入数据时，hbase会自动创建插入数据的索引吗？

Apache Hive 是一个数据仓库系统，用于查询和管理大型数据集，而 Apache HBase 是一个分布式、可扩展的大数据存储。Hive 可以通过自定义存储处理器（Storage Handler）来与 HBase 集成，使得你可以通过 Hive 来查询 HBase 中的数据。

要在 Hive 上对接 HBase 中的表，你需要使用 Hive 提供的 HBaseStorageHandler。以下是一个简单的例子，展示了如何在 Hive 中创建一个映射到 HBase 表的外部表：

CREATE EXTERNAL TABLE hive_table_name(key int, value string)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,cf:value")
TBLPROPERTIES ("hbase.table.name" = "hbase_table_name");
在这个例子中，hive_table_name 是 Hive 表的名称，hbase_table_name 是 HBase 表的名称，cf 是 HBase 中的列族。

对于你的问题：

当你在 HBase 上更新数据时，Hive 表是否会增加数据？

如果你在 HBase 中插入新的行或更新现有行，这些更改会即时反映在 Hive 的查询结果中，因为 Hive 表只是 HBase 数据的一个视图。但请注意，Hive 不是为实时查询优化的，可能会有一些延迟。

当你在 Hive 表导入数据时，HBase 是否会自动创建插入数据的索引？

当你往 Hive 表中插入数据时，这些数据实际上是被写入到 HBase 表中的。因为 Hive 表是通过 HBaseStorageHandler 映射到 HBase 表的，所以插入操作会转化为 HBase 的 Put 操作。然而，HBase 默认不会为数据创建索引；它只是根据 Row Key 存储和检索数据。如果你需要在 HBase 中创建二级索引，你需要使用额外的工具或自己实现逻辑来维护索引。

请记住，要使得 Hive 与 HBase 集成，你需要在 Hive 的 classpath 中包含 HBase 和 ZooKeeper 的配置文件，并且确保 Hive 能够访问运行 HBase 的 Hadoop 集群。此外，Hive 与 HBase 的集成通常用于批量处理，而不是实时查询，所以在设计系统时应考虑数据延迟和读写吞吐量。

那如果我在hbase上对一个rowkey为1数据，更新3次，那么hive查询的数据是只有1条rowkey为1的数据，还是4条rowkey为1的数据

当你在 HBase 中对同一个 rowkey 更新数据时，HBase 会根据其版本管理特性来处理这些更新。HBase 中的每个 cell（由 rowkey、列族、列限定符确定）可以存储多个版本的数据，版本通过时间戳进行区分。

默认情况下，当你对同一个 rowkey 进行更新操作时，HBase 会保留每次更新的版本，除非你在创建表的时候指定了版本的数量（VERSIONS）。如果没有指定版本数量，HBase 默认通常保留一个版本。这意味着新的更新会覆盖旧的数据。

在 Hive 中查询 HBase 表时，Hive 通常只会看到每个 cell 的最新版本的数据。因此，如果你对 HBase 中某个 rowkey 的数据更新了3次，Hive 查询结果中只会有一条 rowkey 为1的数据，并且该数据将是最新版本的数据。

如果你需要在 Hive 中查询 HBase 的旧版本数据，你需要在创建 Hive 表时指定版本信息。但是，这通常不是 Hive 使用的典型用例，因为 Hive 主要用于处理最新的数据快照，而不是用于查看数据的历史版本。如果你需要处理多个版本的数据，可能需要在 HBase 中直接操作或使用其他支持版本控制的系统。
```

##### 方案2缺点

```

我用hive创建A表去关联hbase的H表，当我用hive中其他表和Ajoin处理时需要获取A 所有数据。这样执行mr程序时，是通过hbase服务来获取A表所有数据吗？这样合理吗？是否我应该把hbase中的表，重新写一份到hive的B表中，对B表进行处理呢，这样底层分解mr程序时，是通过b表的hdfs文件来获取数据的，而不是通过hbase查询来获取数据

在Hive中创建的表A，如果是作为HBase的外部表，那么当你执行与表A相关的join操作时，Hive会通过HBase的API来访问HBase中的H表数据。这意味着，数据的读取是通过HBase服务进行的，而不是直接从HDFS读取。
使用HBase作为数据源有以下几点考虑：
实时性：如果你的应用场景需要实时查询或者频繁的小范围更新，HBase提供的实时读写能力是HDFS无法比拟的。
随机读写：HBase支持随机读写访问，这对于某些应用场景ßß是非常有用的。
扩展性：HBase可以通过增加更多的RegionServers来水平扩展。
然而，如果你的操作主要是大批量的数据处理，尤其是需要全表扫描的场景，直接在Hive中处理数据可能会更加高效。因为Hive是为了优化MapReduce作业而设计的，它可以直接在HDFS上高效地进行大规模数据的批处理。
如果你发现通过HBase进行数据访问导致性能问题，或者你的使用场景更倾向于批处理而不是实时查询，那么将数据从HBase导出到Hive中的普通表（表B），然后对这个普通表进行处理可能是一个好主意。这样做有以下好处：
性能：Hive直接在HDFS上操作数据，对于批量数据处理更加高效。
成本：MapReduce作业通常会消耗较多资源，如果能够在HDFS上直接处理数据，可能会减少资源的使用，从而降低成本。
简化查询：对于Hive来说，直接处理存储在HDFS上的数据通常比通过HBase API访问数据要简单。
不过，这样做的缺点是：
数据冗余：你将在HBase和HDFS中维护同样的数据集，这会增加存储成本。
数据一致性：你需要确保HBase中的数据和导出到HDFS的数据保持一致，这可能需要定期的数据同步。
综上所述，是否需要将数据从HBase导出到Hive，取决于你的具体需求，包括查询的实时性、更新的频率、以及处理的数据量大小。通常，对于需要高性能批处理的场景，将数据从HBase同步到Hive中的HDFS文件系统，然后在Hive上进行分析是一个不错的选择。
```

##### 方案2解决方案(待定)

```sql
#需解决问题
hbase全表扫描性能问题
hbase表执行hql的并行度问题
多个hql任务查询hbase同一个全表数据，服务器能不能扛得住
flink任务故障，怎么恢复，确保数据没问题。
如何精准一次消费，故障恢复会不会出现重复数据



#全表扫描性能问题
hbase这个问题可以通过建表时预分区解决，hbase是顺序写入，如果预分区和rowkey设计好，就不会有个问题了

#并行度问题

在Hive中执行与HBase的关联操作时，MapReduce任务的数量并不是静态确定的，而是由多个因素决定的，包括：

1. **Hive配置**：Hive中有多个设置可以影响MapReduce作业的并行度，例如`hive.exec.reducers.bytes.per.reducer`（每个reducer处理的数据量）、`mapreduce.job.reduces`（reducer的数量）等。

2. **数据分布**：如果你的Hive表和HBase表都很大，Hive将尝试在Map阶段分布处理这些数据。Hive表的Map任务数量通常取决于表的分区和HDFS上文件的数量和大小。而HBase的并行度可能取决于表的region数量。

3. **HBase集成**：在Hive与HBase集成时，HBase表通常被看作一个外部表。读取HBase表的数据通常不会通过MapReduce任务直接完成，而是通过Hive与HBase的连接器实现的。这意味着，对HBase表的直接查询可能不会触发MapReduce任务。但是，当你执行JOIN操作时，通常Hive表的数据会通过MapReduce处理，而HBase的数据可能通过查找操作获取。

4. **查询计划和优化**：Hive会为每个查询生成一个执行计划，这个计划取决于查询的复杂性和Hive优化器的决策。优化器可能会决定合并多个操作以减少MapReduce作业的数量，或者分解一个复杂的查询为多个步骤。

因此，具体的Map任务数量需要根据你的查询、数据、Hive配置和HBase的设置来确定。你可以通过查看Hive执行查询时生成的执行计划来获取这些信息。执行计划会告诉你Hive准备启动多少个Map任务和Reduce任务，以及它们的配置和执行顺序。

为了得到一个更准确的答案，你可以运行一个EXPLAIN命令，这样Hive就会展示出它对于特定查询的执行计划，包括MapReduce任务的数量和执行的步骤。例如：


EXPLAIN SELECT ...
FROM hive_table
JOIN hbase_table ON ...
这将给出一个详细的执行计划，你可以从中了解到MapReduce任务的数量和其他相关信息。


```



### 方案3:iceberg/hudi

```sql
Apache Iceberg 和 Apache Hudi 都可以与 Hive 集成，但它们的集成方式和程度有所不同。下面是对两者与 Hive 集成情况的简要说明：

### Apache Iceberg

Apache Iceberg 可以与 Hive 集成，允许你在 Hive 中创建和管理 Iceberg 表。Iceberg 提供了 Hive 的存储处理器插件，这意味着你可以通过 Hive 来查询和管理 Iceberg 表。当你在 Iceberg 表上执行更新、删除或插入操作时，这些变更会被 Iceberg 跟踪，并且可以通过 Hive 查询这些表来感知到这些变更。

为了实现这一点，你需要在 Hive 中设置一些配置，以便它知道如何与 Iceberg 表交互。在 Hive 3.x 之后的版本中，Iceberg 集成得更加深入，因此如果你使用的是较新版本的 Hive，你将能够更加无缝地使用 Iceberg。

### Apache Hudi

Apache Hudi 也可以与 Hive 集成。Hudi 提供了一个 Hive 同步工具，它可以将 Hudi 管理的表同步到 Hive 元数据中。这意味着当你在 Hudi 表上执行写操作（如插入、更新、删除）时，Hudi 会处理数据文件的物理变更，并通过同步工具更新 Hive 元数据，以便 Hive 能够感知到这些变更。

Hudi 表可以通过 Hive 外部表的形式来查询，但是对 Hudi 表的更新和删除操作通常是通过 Hudi 的API或与 Hudi 集成的计算引擎（如 Flink、Spark）来执行的，而不是直接通过 Hive。

### 注意事项

- **数据感知**：对于通过 Iceberg 或 Hudi 管理的表，Hive 可以感知到更新和删除操作，但是这些操作不是通过 Hive 的 ACID 事务来执行的，而是通过各自的事务机制。
  
- **查询兼容性**：尽管 Hive 可以查询由 Iceberg 或 Hudi 管理的表，但是某些 Hive 特性可能不完全兼容，这取决于你使用的 Hive 版本和配置。

- **版本管理**：Iceberg 和 Hudi 都提供了强大的版本管理和时间旅行功能，这意味着你可以查询到表的历史快照。

- **工具选择**：选择使用 Iceberg 还是 Hudi 可能取决于你的特定需求，如对时间旅行的需求、对不同计算引擎的支持、以及对性能的具体要求等。

在实施这些集成之前，建议详细了解 Iceberg 和 Hudi 与 Hive 的兼容性，并测试它们在你的环境中的表现，以确保它们满足你的业务需求。
```



# 其他



1.彩贝壳公司，干过的项目。

前半年是数据需求拉取



维护日常数据需求拉取 到至今



构建新的数据仓库模型，任务迁移 到至今

最大的问题是，任务太多了，有一些任务指标已经不知道有没有人去看了，

对于这种累计型任务，建议设置一个最近多少个月没人观看，然后终止任务，节省集群功能。

因为报表系统问题，待完成。



集群数据质量监控问题监控

有时候总是出现数据数据少，原因很多，后端落的不合理，新模型逻辑变动覆盖不到。

变动沟通不及时，没人漏斗查看数据减少，超出规模，将异常数据找出，超出规模报警。



对接算法，实时推荐系统。



# 框架详细

#### kafka

```sql
#怎么合理设置topic分区数

#消息积压怎么处理

#精准一次性消费

#乱序问题怎么处理
```

#### hadoop

```sql
#简历内容hadoop详细到点,
```

