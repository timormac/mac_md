# 项目详细

## 实时推荐项目

改架构模型:  前端埋点，原本是批发送，或者用户下线时批量发送，点一下发一下。后端下订单直接发送到kafka消息队列。沟通







## 数据仓库离在线混搭

离在线混搭和实时数据仓库区别：

​	个人理解，离线在线混搭，解决的问题，是大量任务在凌晨执行，对于数据查询的实时性没那么高，只是为了提高白天资源利用率，

​    用不到毫秒级别的响应。离在线混搭是为了减少一些当天数据的处理时间，对于历史数据和涉及到历史统计的，还是要用离线仓库去  	做，不可能用实时任务去完全替代。并且一些很复杂的hql，clickhouse没发处理，比如流失率等等，7天内连续登陆问题，复购率

 

​	而实时仓库，是为了毫秒级别的响应，查看app，当天运营状态，所以要引入clickhouse框架等。



​	

#### 需要解决问题

flink写hive要解决小文件问题。

最开始hive不支持更新操作，类似订单表只能当天最后状态。如果想用实时来做，无法完成对应分区更细修改数据。

因为hdfs只支持增量写入，不支持修改。但是hive新版有了ACID后，对于修改的数据，可以支持了。

现在又遇到的问题就是,flink不支持ACID写入。flink是针对hive表的元数据格式，直接对hdfs文件做处理的。

这样才能保证快，因为不是通过hive写入，所以没法完成hive的ACID。通过hive就算写一条数据，也要走mr这种效率太低了。

所以现在问题卡在这里了。

如果把hive表用hbase来绑定，也有个问题，就是当大量job任务来来查询是，hbase能不能向mr一样，同时支持并快速的响应数据支持。因为hive绑定hbase是通过hbase服务器来响应数据的，而不像hive直接大家拿hdfs文件就行。



如果想最佳的解决这个问题,那么了解hiveACID的原理，通过flink手动维护ACID文件来完成修改数据。

简单的解决思路，flink对修改的数据进行缓存,定期通过hive客户端插入到hive表中，走mr。

查询了些资料：hudi在这方面挺合适的，也能和hive对接，和habase有点像，不过没搞，用其他方案解决的。

hudi介绍

```
Apache Hudi 和 Apache HBase 都是大数据生态系统中的重要组件，但它们是为了不同的用例和场景设计的。以下是它们之间的一些关键区别，以及在何种场景下可能会选择 Hudi 而不是 HBase：

### Apache Hudi

- **用例**：Hudi 主要用于构建和管理数据湖，它支持在大规模数据集上进行高效的插入、更新和删除操作。Hudi 适用于需要处理批量和流数据的场景，特别是在数据仓库和数据湖架构中。
- **存储模型**：Hudi 存储在HDFS或兼容的文件系统（如Amazon S3）上，使用文件作为数据存储的基本单位，并且支持列式存储格式，如Parquet和ORC，这对于分析查询是优化的。
- **一致性和事务**：Hudi 提供了快照隔离和事务支持，使得在大数据环境中进行复杂的数据管理成为可能。
- **实时性**：Hudi 支持近实时的数据处理和查询，特别适合需要频繁更新数据集的场景，如数据湖实时同步、增量ETL管道和流处理。

### Apache HBase

- **用例**：HBase 是一个分布式、可扩展的NoSQL数据库，设计用于处理随机、实时的读/写访问大型数据集。HBase 适用于需要低延迟访问的在线服务，如用户界面后端、实时分析和监控系统。
- **存储模型**：HBase 在 HDFS 上以键值对的形式存储数据，是一个面向列的数据库，适合处理大量的非结构化或半结构化数据。
- **一致性和事务**：HBase 提供了强一致性的读写能力，但不支持跨多行的事务。
- **实时性**：HBase 优化了低延迟的数据访问和更新，适合需要快速响应的应用程序。

### 场景对比

以下是选择 Hudi 而不是 HBase 的一些具体场景：

1. **数据湖同步**：如果你正在构建一个数据湖，并且需要处理来自不同数据源的数据，同时又需要支持数据的插入、更新和删除操作，Hudi 是一个更好的选择。Hudi 可以轻松地与现有的数据仓库解决方案（如Hive）集成，支持复杂的ETL作业和增量数据处理。

2. **分析工作负载**：对于需要高效进行批量分析查询的场景，Hudi 提供了基于列的存储格式（如Parquet），这对于分析型查询是优化的。HBase 的行存储模型并不适合批量分析工作负载。

3. **变更数据捕获（CDC）**：如果你需要实现变更数据捕获，以便将数据库的变更实时同步到数据湖中，Hudi 提供了更好的工具和机制来处理这种类型的工作流。

4. **增量数据处理**：在需要增量读取和处理数据的场景下，Hudi 提供了增量查询的能力，这使得只处理自上次查询以来发生变化的数据成为可能，从而提高效率。

例如，如果一个电子商务公司想要实时分析用户的交易数据，并且这些数据存储在数据湖中，同时需要能够处理订单更新或取消，那么使用 Hudi 将是更合适的选择。Hudi 可以确保数据的实时更新，并且可以直接集成到现有的分析工具和查询引擎中，如Hive和Spark。而如果这个公司需要一个用户行为的实时推荐系统，可能会选择 HBase 来存储和查询用户的点击流数据，因为它可以提供快速的随机访问性能。
```



使用hbase表代替hive

需要测试下，如果对hbase数据查询，做开窗等操作，是在hive中做，还是在hbase中做。

或者获取全量数据难道是通过hbase客户端一条条查询然后输出出去吗。。。

```
hive中A表绑定一个hbase外部表。当我有很多job任务需要用到A表时，这个job获取a表数据，是通过hbase的客户端查询数据？还是直接通过底层的hdfs文件拿到数据?效率会比直接在hive上创建一个A表来说，效率低吗
2023/12/20 12:30:09

在Hive中绑定一个HBase外部表时，你实际上是在创建一个Hive表的元数据，这个元数据指向了HBase中的表。这意味着，当你通过Hive查询这个外部表时，Hive将使用HBase的客户端API来与HBase交互，获取所需的数据。Hive将转换SQL查询为相应的HBase操作，比如get和scan。
因此，当你的job任务需要使用这个绑定了HBase的A表时，数据的获取是通过HBase客户端与HBase服务器通信来完成的，而不是直接从HDFS文件中读取。这种方式的效率与直接在Hive上创建一个内部表并直接从HDFS读取数据相比，通常会有所不同：
延迟: HBase是为了优化低延迟的随机读写而设计的，而Hive通常用于批量处理大量数据。如果你的查询是随机访问少量的行，HBase可能会提供更好的性能。但是，如果你要扫描大量的数据，直接通过Hive访问HDFS可能会更快，因为这样可以利用Hive的优化，比如列式存储格式（如Parquet或ORC）。
资源使用: 通过HBase客户端API访问数据意味着你的查询会通过HBase的RegionServers进行，这可能会影响HBase集群的性能，尤其是当有大量的并发查询时。而Hive直接访问HDFS则主要受到Hadoop集群资源的限制。
数据局部性: HBase优化了数据的局部性，这意味着相关数据通常存储在一起，这对于某些类型的查询非常有效。但是，Hive查询通常是针对大量数据的，数据局部性的优势可能就不那么明显了。
复杂查询: Hive支持的SQL功能比HBase强大，对于复杂的分析查询，Hive可能会更加适合。
在性能方面，没有一个统一的答案，它取决于你的工作负载特性和你的集群配置。在某些情况下，直接在Hive上创建内部表并从HDFS读取数据可能会更快，特别是对于大规模的批处理作业。对于需要频繁的小范围查找的作业，使用HBase可能会更有优势。
为了最佳性能，你可能需要根据你的具体使用情况进行基准测试，比较直接使用Hive内部表和通过HBase外部表访问数据的性能差异。
```





#### 设计理念

最开始hive不支持更新操作，类似订单表只能当天最后状态。如果想用实时来做，无法完成对应分区更细修改数据。

因为hdfs只支持增量写入，不支持修改。但是hive新版有了ACID后，对于修改的数据，可以支持了。

所以有了实时处理dwd数据的想法。 

现在又遇到的问题就是,flink不支持ACID写入。flink是针对hive表的元数据格式，直接对hdfs文件做处理的。

这样才能保证快，因为不是通过hive写入，所以没法完成hive的ACID。通过hive就算写一条数据，也要走mr这种效率太低了。

所以现在问题卡在这里了。







#### 方案1：

flink直接写入hive表上，这个夭折了，开发难度大，自己维护幂等性难，故障恢复处理难，hive中acid事务表无法处理。

#### 方案2:

 flink写入hbase表，通过hive对接hbase表。但是hive表联通hbase表后，就无法使用 分区功能了，所以为了能够快速定位当日数据，要为hbase添加dt字段索引，并且这样也不会改变下一层的用到这张表的任务。

用hbase还有些优化的地方，比如hive使用hbase表的原理。hive用hbase表的话，执行mr任务，还能多平行度吗？如何多并行度多，还能手动指定map数吗等等。



**方案2实现**

```
有什么可以帮你的吗

我在hive上如何对接hbase中的表，当我在hbase上更新数据时,hive表会增加数据吗？当我在hive表导入数据时，hbase会自动创建插入数据的索引吗？

Apache Hive 是一个数据仓库系统，用于查询和管理大型数据集，而 Apache HBase 是一个分布式、可扩展的大数据存储。Hive 可以通过自定义存储处理器（Storage Handler）来与 HBase 集成，使得你可以通过 Hive 来查询 HBase 中的数据。

要在 Hive 上对接 HBase 中的表，你需要使用 Hive 提供的 HBaseStorageHandler。以下是一个简单的例子，展示了如何在 Hive 中创建一个映射到 HBase 表的外部表：

CREATE EXTERNAL TABLE hive_table_name(key int, value string)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,cf:value")
TBLPROPERTIES ("hbase.table.name" = "hbase_table_name");
在这个例子中，hive_table_name 是 Hive 表的名称，hbase_table_name 是 HBase 表的名称，cf 是 HBase 中的列族。

对于你的问题：

当你在 HBase 上更新数据时，Hive 表是否会增加数据？

如果你在 HBase 中插入新的行或更新现有行，这些更改会即时反映在 Hive 的查询结果中，因为 Hive 表只是 HBase 数据的一个视图。但请注意，Hive 不是为实时查询优化的，可能会有一些延迟。

当你在 Hive 表导入数据时，HBase 是否会自动创建插入数据的索引？

当你往 Hive 表中插入数据时，这些数据实际上是被写入到 HBase 表中的。因为 Hive 表是通过 HBaseStorageHandler 映射到 HBase 表的，所以插入操作会转化为 HBase 的 Put 操作。然而，HBase 默认不会为数据创建索引；它只是根据 Row Key 存储和检索数据。如果你需要在 HBase 中创建二级索引，你需要使用额外的工具或自己实现逻辑来维护索引。

请记住，要使得 Hive 与 HBase 集成，你需要在 Hive 的 classpath 中包含 HBase 和 ZooKeeper 的配置文件，并且确保 Hive 能够访问运行 HBase 的 Hadoop 集群。此外，Hive 与 HBase 的集成通常用于批量处理，而不是实时查询，所以在设计系统时应考虑数据延迟和读写吞吐量。

那如果我在hbase上对一个rowkey为1数据，更新3次，那么hive查询的数据是只有1条rowkey为1的数据，还是4条rowkey为1的数据

当你在 HBase 中对同一个 rowkey 更新数据时，HBase 会根据其版本管理特性来处理这些更新。HBase 中的每个 cell（由 rowkey、列族、列限定符确定）可以存储多个版本的数据，版本通过时间戳进行区分。

默认情况下，当你对同一个 rowkey 进行更新操作时，HBase 会保留每次更新的版本，除非你在创建表的时候指定了版本的数量（VERSIONS）。如果没有指定版本数量，HBase 默认通常保留一个版本。这意味着新的更新会覆盖旧的数据。

在 Hive 中查询 HBase 表时，Hive 通常只会看到每个 cell 的最新版本的数据。因此，如果你对 HBase 中某个 rowkey 的数据更新了3次，Hive 查询结果中只会有一条 rowkey 为1的数据，并且该数据将是最新版本的数据。

如果你需要在 Hive 中查询 HBase 的旧版本数据，你需要在创建 Hive 表时指定版本信息。但是，这通常不是 Hive 使用的典型用例，因为 Hive 主要用于处理最新的数据快照，而不是用于查看数据的历史版本。如果你需要处理多个版本的数据，可能需要在 HBase 中直接操作或使用其他支持版本控制的系统。
```





**方案1**

```sql
#哪些层哪些表能用flink代替
ods可以，dwd待定，待思考。

用分区方法遇到的问题，有的表没有分区，改成分区表，则相关依赖的sql都用改。
所以对于没分区的表，继续用原来的表，这种表通常不台大。

flink没法读写acid表，自己要维护事务插入，也没法对接hive的acid因为不知道实现原理。
代码开发难度太大，注意问题太多，重复数据处理，故障数据恢复等。
只对几个主表进行这么操作，主业务表，其他支线都根据主表来跑，其他的是能并行的。


#凌晨读取数据，锁表问题
肯能导致数据写入不进去，需要去试。
解决方法：
1 分区，这里有个问题，要更改原hive任务的hql
2 hive的ACID表

用分区方法遇到的问题，有的表没有分区，改成分区表，则相关依赖的sql都用改。
所以对于没分区的表，继续用原来的

#要想实现dwd层，数据更新问题
hive新版本的ACID表，现在读取性能更好，用ACID表来做。
flink不支持读取和写入acid表

#kafka数据的精准一次性消费问题
用flink写入kafka一次性消费，详情看flink.md。代码实现复杂，并且影响吞吐量。
最简洁的方法是用table_api来写,然后出现问题了，手动去分区重跑数据。
tableAPI好像也能实现精准一次消费。


#任务故障，怎么数据恢复
正常情况重启就行，kafka数据继续消费。
如果真的出现数据条数不同，数据多了或者丢了，用原来的离线脚本执行问题分区。

#dwd改写，开发难度大，耗时高
本来hive就是为了加快速度的，减少mr编写。
如果实现了dwd改写，那也很复杂，想象怎么能提高dwd改写速度。
目前的思路是每小时执行一次，然后12点合并，但是怕有的表会改动数据，或者关联不到的情况。待处理

#什么时候接入系统的
把以前离线仓库的脚本任务的主任务，涉及到的表用flink任务重新编写，每天对比数据，数据稳定之后，替换表数据来源。

#怎么分配内存使用的
压侧，顶峰等具体看flink优化视频里有

```



# 其他



1.彩贝壳公司，干过的项目。

前半年是数据需求拉取



维护日常数据需求拉取 到至今



构建新的数据仓库模型，任务迁移 到至今

最大的问题是，任务太多了，有一些任务指标已经不知道有没有人去看了，

对于这种累计型任务，建议设置一个最近多少个月没人观看，然后终止任务，节省集群功能。

因为报表系统问题，待完成。



集群数据质量监控问题监控

有时候总是出现数据数据少，原因很多，后端落的不合理，新模型逻辑变动覆盖不到。

变动沟通不及时，没人漏斗查看数据减少，超出规模，将异常数据找出，超出规模报警。



对接算法，实时推荐系统。



# 框架详细

#### kafka

```sql
#怎么合理设置topic分区数

#消息积压怎么处理

#精准一次性消费

#乱序问题怎么处理
```

#### hadoop

```sql
#简历内容hadoop详细到点,
```

