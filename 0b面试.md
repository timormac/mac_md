# 简历

1-3年和3-5年简历写的内容不一样



3-5年的写法

```
项目经验：
分布式实时分析系统 | [公司名称] | [项目时间]项目简介： 领导一个跨部门团队，设计并实现了一个分布式实时分析系统，旨在处理和分析来自全球多个数据源的数据流，提供即时的业务洞察和预测分析。
技术栈： Hadoop, Kafka, Flink, HBase, MySQL, Flume, Sqoop, Hive
责任：作为项目技术负责人，设计系统架构，确保可扩展性和高可用性。
领导数据工程团队，实施了端到端的数据流解决方案，包括数据收集、存储、处理和分析。
优化了数据处理流程，通过自动化和精细化管理提高了数据吞吐量和降低了延迟。
协调开发和运维团队，通过持续集成和持续部署（CI/CD）实践减少了系统故障率。
引入了新的监控和日志分析工具，提高了系统的透明度和问题解决速度。
成就：成功处理日均上百亿条数据记录，提升了数据处理速度和准确性，为实时决策提供了支持。
通过系统优化，将数据处理延迟降低了40%，显著提高了用户满意度。
设计的系统支持了新的数据驱动产品的开发，为公司创造了新的收入流。
证书：
```



# 项目(github上找)

### 实时推荐项目

不要直接写实时数仓的建设

flink项目用github上的实时推荐系统，对接算法提供数据。

```sql
#项目方案设计

#测试总链路时间，以及下单接口的节省链路时间
看看算法团队，从获取数据到算出数据，需要节省的时间。有没有必要去在下单接口直接kafka节省。如果有必要再做。
就省了写入数据库的时间和maxwell抓捕数据的时间

#链路优化
因为对实时性需求很高，原本方案是,maxwell监控mysql写到kafka。为了加快链路直接后端的下单接口写入kafka
不过不敢加，怕对原有业务造成印象
问题：
他们下单用的mq和kafka的mq不一样，有的mq不支持。

```



下单接口加kafka需要解决的问题

```
您提到的方案，即在商品下单接口处直接将数据推送到Kafka，是一种常见的实践，尤其是在构建实时数据流和推荐系统时。这种方案可以减少数据传输链路中的延迟，提高数据处理的实时性，因为它避免了监控数据库变化并将数据变更捕捉后再传输的过程。
要评估这种改动的合理性和对后端的改动需求量，我们可以从以下几个方面来考虑：
实时性需求：如果您的系统对于数据处理的实时性有较高要求，那么直接在下单接口处推送数据到Kafka是更优的选择。
系统架构：如果已有的系统架构允许在下单接口处加入消息队列的推送机制，而且对其他服务的影响不大，这种改动是可行的。
后端改动：
代码修改：后端需要添加将订单数据推送到Kafka的逻辑。这通常涉及到修改下单服务的业务逻辑代码，以及可能的错误处理和重试机制。
测试：改动后的代码需要经过充分的测试，包括单元测试、集成测试以及可能的压力测试，以确保新逻辑不会引入错误，并能够在高负载下稳定工作。
部署：修改后的服务需要重新部署，这可能涉及到CI/CD流程的调整。
性能考虑：直接在下单接口推送数据到Kafka可能会对下单接口的性能产生影响。如果下单接口的负载已经很高，这种改动可能需要更多的性能优化措施。
可靠性与容错：您需要确保新的数据推送机制能够可靠地处理消息，包括网络问题、Kafka不可用时的数据丢失风险等。
监控与日志：改动后，您可能需要更新监控系统和日志策略，以便更好地跟踪数据流和及时发现问题。
团队协作：这种改动需要后端团队与数据团队的紧密合作，确保数据格式、数据流的设计符合算法团队的需求。
综合来看，如果您的团队对Kafka比较熟悉，且后端服务的修改不会引起大规模的架构变动，那么这种改动是合理的，并且可以带来数据处理的速度提升。但是，这需要进行详细的评估和规划，以确保改动后的系统稳定可靠。
```



实时推荐系统对于数据的处理速度和访问速度有非常高的要求。在设计实时数据供给方案时，需要考虑以下几个关键因素：

1. **数据的实时性**：推荐系统需要尽可能快地响应用户的行为，实时更新推荐内容。
2. **系统的扩展性**：随着用户量和数据量的增长，系统应能平滑扩展。
3. **数据的一致性和可靠性**：确保算法团队获取的数据是准确和完整的。
4. **系统的可维护性**：易于监控、调试和故障排除。

基于这些考虑，可以有几种方案供选择：

方案一：消息队列（如 Kafka）

将数据实时推送到一个消息队列中，算法团队可以订阅这个队列来获取数据。这种方式的好处是：

- **高吞吐量和低延迟**：消息队列设计用来处理高速数据流。
- **解耦生产者和消费者**：大数据团队和算法团队可以独立工作，不会相互阻塞。
- **容错性好**：消息队列通常提供数据持久化，即使系统出现问题，也不会丢失数据。

方案二：实时数据库/数据仓库（如 Apache Druid）

将数据实时写入支持实时查询的数据库或数据仓库中，算法团队可以通过SQL查询来获取数据。这种方式的好处是：

- **即时查询**：支持复杂的查询操作，算法团队可以灵活地获取所需数据。
- **易于集成**：很多实时数据库提供良好的接口支持，易于与现有系统集成。

方案三：实时API接口

大数据团队提供一个实时API，算法团队可以通过这个API按需拉取数据。这种方式的好处是：

- **按需访问**：算法团队可以根据需要获取数据，避免不必要的数据传输。
- **易于控制和管理**：API的使用可以通过权限和配额进行细粒度的控制。

方案四：共享内存/内存数据库（如 Redis）

使用内存数据库作为缓存层，实时同步数据，算法团队可以快速访问这些数据。这种方式的好处是：

- **访问速度快**：内存数据库提供极低的延迟数据访问。
- **支持数据结构丰富**：可以根据需求使用适合的数据结构来优化性能。



推荐方案

对于实时推荐系统，我倾向于推荐**组合使用消息队列和实时数据库/数据仓库**的方案。具体来说：

1. **消息队列**：Flink实时处理数据后，将结果推送到消息队列（如Kafka）中，这样可以确保数据的实时性和系统的高可用性。
2. **实时数据库/数据仓库**：同时，将处理后的数据实时写入一个实时数据库（如Apache Druid）或数据仓库中，算法团队可以通过实时查询来访问历史数据和实时数据，进行更复杂的数据分析和挖掘。

结合这两种技术，可以让算法团队根据需要选择最合适的方式来访问数据，无论是实时消费消息队列中的数据，还是通过实时数据库进行复杂查询，都可以得到支持。同时，这种方案具有较好的扩展性和容错性，可以随着系统的增长进行相应的扩展。









### 离线数仓更新项目(离线在线混搭)

对于原来的sql也做了优化，之前任务，更新1条就要重写整个订单表的分区业务数据。

需解决问题:			

​			程序挂了，数据怎么保持一致。脚本校对数量，短信通知。

​			因为实时任务一直再跑而且记录到消费到哪的数据了，人工介入比较难

​			或者更改flink代码，更改过滤掉某时间点之内的数据。flink第一次也要这样。



ods层可以实时做，最简单的。

dwd层如何实时做呢？？？？会遇到哪些场景呢？

因为表关联之后，会有变化数据，所以怎么才能实时做。多次join，同一条数据，1-24小时的表，每次都汇总前面表，然后取最大分区的。这样就能做了，随着dwd改了，dws也要更新的。



当凌晨查询flink同步的表时，会锁表吗？如果这时候flink数据插入，导致数据进不去怎么办的？

对于分区表还好，因为flink凌晨写入的是第二天的分区了。非分区表，只有一张的表，就锁表了

```sql
Hive 本身是建立在 Hadoop 的 HDFS 之上的，并不是一个真正意义上的事务型数据库，它的设计初衷是用来做批处理的，所以它的表锁定机制并不像传统关系型数据库那样严格。

对于 Hive 来说，它的 "锁" 更多的是在 Hadoop 的文件系统层面上。Hive 1.2.0 版本之后引入了一些轻量级的事务处理和行级别的锁定，但这些通常用于支持 ACID（原子性、一致性、隔离性、持久性）操作，需要开启 Hive 的事务功能，并且使用支持事务的表格式（如 ORC 格式）。如果你的 Hive 表没有启用事务特性，那么它基本上不会有类似关系型数据库中的锁表行为。

然而，当 Hive 正在读取数据时，如果 Flink 任务尝试写入相同的数据文件，可能会遇到并发问题。这是因为 HDFS 是一个写一次读多次的文件系统，不支持对文件的并发写操作。如果 Flink 任务在写入时文件被 Hive 任务读取，可能会导致写操作失败。

为了处理 Hive 和 Flink 之间的并发问题，你可以考虑以下几种策略：

时间窗口隔离：安排 Flink 实时写入作业和 Hive 批量读取作业在不同的时间窗口执行，以避免并发冲突。

基于分区的写入：如果使用的是分区表，Flink 可以写入一个新的分区，而不是正在被 Hive 查询的分区。这样，Hive 查询时不会读取 Flink 正在写入的分区。

使用临时表：Flink 写入数据到一个临时表，然后在 Hive 的低峰时段将这些数据批量转移到主表中。

流式处理与批处理分离：考虑将流处理和批处理的数据分开存储，流处理写入实时表，批处理从历史数据表读取，通过定期的作业将实时表的数据合并到历史数据表中。

使用 ACID 表：如果你的 Hive 版本支持，并且你的用例可以容忍相关的性能开销，可以考虑使用 Hive 的事务表功能。这将允许 Hive 和 Flink 使用行级锁，从而更好地支持并发操作。

数据湖解决方案：考虑使用更先进的数据湖解决方案，如 Delta Lake 或 Apache Hudi，这些解决方案提供了更好的并发控制和事务管理。

在实施上述任何策略之前，重要的是要了解你的数据访问模式，并对 Hive 和 Flink 的交互进行充分的测试，以确保你的数据管道的稳定性和可靠性。
```



dws基于dwd层，dwd不跑完没发弄。



2套体系同时再跑，然后对数，稳定知道，老系统停下来。对于有时特别小的误差，不要了，问题不好找，用于分析。









# 公司业务数据

### 彩贝壳类型

```
美团是一个综合性的生活服务电子商务平台，主要提供包括餐饮外卖、酒店旅游预订、电影票务、到家服务、共享单车等在内的本地生活服务。美团的业务模式是典型的C2B2C（Consumer to Business to Consumer），P2P通常指的是个人与个人之间的直接交易，没有商家的介入。
彩贝壳自己采购了许多产品，所以应该算是B2C


商务团队（或商家服务团队）：负责与商家建立合作关系，帮助商家在平台上成长，提供营销和广告服务等。这个团队有时也被称为销售团队或BD（Business Development）团队。
```



### 公司100w日活数据

```sql
电商平台的用户活跃度、新增用户数、每日订单数以及下单率这些指标受到多种因素的影响，比如平台的市场定位、用户群体、产品种类、季节性因素、促销活动等。不过，我可以提供一些行业平均水平的参考数据，但请注意这些数据仅供参考，实际情况可能会有所不同。

1. 每日新增用户数：对于一个日活跃用户数(DAU)为100万的成熟电商平台，如果是在正常运营阶段而非大型促销期间，每日的新增用户可能会占日活跃用户的一小部分。一个粗略的估计可能在0.5%到2%之间，这意味着每天的新增用户数可能在5000到20000之间。

2. 每日订单数：每日订单数与平台的转化率密切相关。转化率是指访问电商平台的用户中有多少比例会下单。对于一些成熟的电商平台，转化率可能在1%到5%之间。以2%的中等转化率计算，如果日活跃用户数为100万，那么每日的订单数可能在2万到5万之间。

3. 下单人数占活跃人数的比率：这个比率实际上就是上面提到的转化率。一个电商平台被认为是表现良好，如果其转化率能够达到3%以上。不过，这个数字对于不同的电商平台和市场来说差异很大。例如，一些高度专业化或者提供高价值商品的平台可能会有更低的转化率，而一些面向快速消费品市场的平台可能会有更高的转化率。

请记住，这些数字只是基于行业平均水平的大致估计，并不代表任何特定电商平台的实际情况。您应该根据自己公司的具体情况进行分析和比较。
```



### 10w 日活

每日3000单，转化率3%  新增用户数是活跃的0.5% :300-500人

客单价是150元,利润10%  一个月135w 。

```
在中国，一个拥有10万日活跃用户（DAU）的电商平台可以认为是一个中等规模的平台。中国的电商市场非常庞大，由几个巨头如阿里巴巴的天猫和淘宝、京东、拼多多等主导，它们的日活跃用户数以亿计。但是，10万日活跃用户对于一个专注于特定细分市场或者新兴的电商平台来说是一个不错的成绩，可能意味着这个平台在它的细分市场中具有一定的影响力和用户基础。

至于具体的运营数据，这些数字会根据平台的业务模式、用户群体、市场定位等因素而有很大的差异。以下是一些非常粗略的估计，仅供参考：

1. **每日新增用户数**：这个指标受多种因素影响，包括市场扩张速度、品牌知名度、营销活动等。对于一个成熟的平台，每日新增用户数可能在千到数千不等。新兴平台可能会有更高的比例，特别是在推广期。

2. **每日下单量**：这取决于用户的购买频率和平台的商品种类。一般来说，电商平台的转化率（即浏览到购买的转化）可能在1%到10%之间。假设转化率为3%，那么10万日活跃用户可能会产生大约3000单的日订单量。

3. **用户流失率**：这是衡量用户停止使用服务的速度。对于电商平台，年流失率可能在10%到50%之间，具体取决于用户黏性和市场竞争。如果按月计算，流失率可能在1%到4%之间。但这些数字可能因平台而异，并且在不同的成长阶段也会有所不同。

4. **商家的利润比**：商家的净利润率取决于他们的成本结构和定价策略。在电商平台上，商家的利润通常是GMV（商品交易总额）的一部分。利润率可能在5%到20%之间，但这会因行业、产品类型、品牌定位等不同而有很大差异。

需要注意的是，这些数字仅供参考，实际情况会根据具体的平台和市场条件有很大的不同。要获得精确的数据，通常需要访问公司的内部运营数据或者行业报告。
```



### 运营数据

统计新增用户来源，微信赠送功能注册的新用户，未知来源注册的新用户数。

如果微信赠送功能，增加新用户，那么就会给大额红包，红包使用率等问题。

### DAu和WAU

```
日活跃用户（DAU）、周活跃用户（WAU）、月活跃用户（MAU）和年活跃用户（YAU）是衡量电商平台用户活跃度的常用指标。这些指标通常用于分析用户参与度、平台粘性和用户忠诚度。

要从日活跃用户（DAU）估算周活跃用户（WAU）、月活跃用户（MAU）和年活跃用户（YAU），我们需要假设或有实际数据关于用户的访问频率。不同的平台和应用场景会有不同的用户行为模式。例如，有的用户可能每天都会访问，而有的用户可能只在周末访问。没有具体的数据，我们只能做一些假设。

WAU/DAU = 1：意味着每天的活跃用户在整个周内都是同一批用户，没有新用户加入，用户粘性极高。
WAU/DAU = 2-3：意味着有一定比例的用户在一周内多次访问，表明有不错的用户粘性和参与度。
WAU/DAU > 3：意味着用户回访率很高，平台在一周内吸引了大量的重复访问，这表明平台运营状况良好，用户粘性很强。
```





# 项目遇到问题

### kafka消息积压项目宕机

```sql
批量更新订单数据，maxwell是监控所有表,导致Kafka多了几百万数据。注意增加字段，不会有upsert的kafka信息
造成影响：
	/* 	监控工具直接短信报警
	   	数据积压了，后续进kafka其他表消息，无法消费到
	   	flink实时任务挂掉了，后台实时面板不更新
	*/
	
	解决方法:
					因为分区3个，只能3个消费者,开多线程。（不知道flink任务能开启多线程吗）
				  线程数是通过zookeeper动态调整的，我把核心线程数调成了8个，核心线程数改成了10个
				  
	复盘:		
					订单系统的批量操作一定提前通知下游系统团队。
					下游系统团队多线程调用订单查询接口一定要做压测。
					对消息积压情况加监控
	
```

### 算法要数据dwt层

算法要用户画像，

用户所有数据，

在线商品说有数据，

商户所有数据

每次看执行任务，最难的就是这个job，

弄了个dwt层，然后出问题，有一次导表，数据异常，有重复数据，然后dwt累加整层就废了。后面dwt分层拆开了，弄个dws日汇总。

注意dws命名的时候，要按功能分域，便于其他同事理解这块sql用途，算法域。

去百度下目前互联网数据仓库分层



### OOM 的解决方法

1  看sql的执行计划，看看有没有优化的地方

   熟悉数据情况，人工给数据分组，再解组

2 手动指定mr数量，增加并行度

3 设置参数jvm虚拟机的Xxm最大堆内存

# 技术栈

1 准备哪些技术栈

​	 永远学不完，准备真实自己项目用到的框架

2  每个技术栈应该怎么写

​	项目用到的核心技术栈，要拆分具体模块，写到的就会问，不会的不要写，问到了不会答就很尴尬

​    非核心技术栈写了解就行，不要拆分模块，仅限于使用，取数等，比如redis

   例子:spark的多模块，比如spark-core,spark-sql,spark-structStreaming没学不要写，然后任务执行流程，rdd原地，shuffle机			   			制,spark调优等。



对于具体的技术栈，可以仔细写一下划分模块,比如hbase读写流程，运行架构，rowkey设计,二级索引，phoneix等

技术栈原则:   写到的都要会，对于一些不常用不熟悉的，直接就写个了解azkaban等，不要展开，只限于写脚本，别的没深入







# 集群规模

群里有个人 28台,192G,32C，80T

阿里云上cpu只有1,2,4,8,16,32,64核的

选的是16cp 128g内存  40T硬盘   5台  一个服务器一台1年是3w 5台15w。



总订单数2000w条,5年，日活待定，日订单待定，新增用户数待定



# flume 

b站视频,flume问的挺多的

https://www.bilibili.com/video/BV1Va4y1X7Am/?spm_id_from=333.999.0.0&vd_source=dc2f0659a9d317ea4b839219ee320ab7

# 数仓



### 脏数据

来源  

1 业务数据或者埋点数据内容中有\t，sqoop导入时没做处理，后端没做非法字符校验

2  老的历史数据模型不一致问题

3 特殊场景下触发的bug，后端没测出来，缺少关键字段数据

4 order表中有的商品id字段应该创建了应该落，不过有的代码没落，他们补充主 订单表拿，但是用的时候用到这个字段

###  模型改善(dwt删除)

后续给删除了，总计用户所有销量，1年内销量等。有错误数据，每次都要洗。

或者把数据按天分区，只洗某一天的。

某一天数据同步出问题了。后续跑批，导致dwt出问题，只能回滚后续几天。		 

### 指标体系

业务数据

商品主题：

​	每日各品牌商家订单数，销售额，购买人数，退单金额，退单数量

​	每日各品类的订单数，销售额，购买人数，退单金额，退单数量

​	每日爆款商品top10，订单数，销售额，购买人数，退单金额，退单数量



交易主题：

​	每日订单数，订单金额，订单人数，退单数，退单人数，退单金额

​	每日各渠道带来的订单金额，订单数量，订单人数(微信公众号，小程序推广，直播，app购买)



用户主题：

​	每日新增用户数，

​	

用户行为数据

用户主题：

​	每日活跃用户数，活跃用户数，回流用户数

​	漏斗分析，每日首页浏览用户数，商品详情页用户数，当人下单人数，支付人数



app设计:

​	各个导航图标的点击数量







数仓分层



dwd层

数据清洗：数据格式不对的统一数据格式：比如日期格式。默认字段空和null转成一致，因为历史表可能不统一

数据加密：手机号等

增加常用字段主键：增加商户id，维护id，签约人员id，避免需要关联时关联多个表来拿指定字段。