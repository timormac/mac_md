# 用户开户断点

### 待确认

```mysql
#kafka zookeeper不对
服务器kafka版本是多少?跳板机的kafka版本是1.10

#总结需求
表二和表三都是按,
ROW_NUMBER() OVER(PARTITION BY c2_v.mobile_tel, c2_v.channel_code order by c2_v.done_time_d) rn
只要mobile_tel，channel_code不变,那么只有一条。
并且第一个表按日期过滤了，跨天的数据会有多条，但是到这里,按照mobile_tel，channel_code,跨天的数据多条也不会再提醒，
这辈子只有一条，因为done_time_d是系统进入时间,第一次超时发过之后，就再也没有了。

#需求明确
如果说视频30分钟，其他8分钟，到底是怎么个逻辑。目前的8分钟情况不对.
比如视频第一个小status，到第二小状态超时30分钟,那么30窗口输出第二个小状态
但是8分钟窗口,在8分钟的时候，就输出第二小状态，但是8分钟窗口给过滤掉了，因为不要视频状态。

？？？感觉这里可能有逻辑漏洞，因为太乱了,理不清




#表二
比如录入视频里面有4个状态码，如果每个状态码之间都超过30分钟,那么会话窗口会输出4条数据。因为都是视频的所以过滤不掉。
然后步骤3按groupby聚合，不知道能不能过滤掉呢？？？

c2_v.client_name,
c2_v.channel_name,
c2_v.branch_name,
c2_v.business_name,
c2_v.channel_code,
c2_v.branch_no,
c2_v.done_time_d,
c2_v.birthday,
c2_v.request_no

因为做了映射,所有视频细分码，统一改成视频见证，所以这里的都是相同的，能保证一条
c2_v.mobile_tel,
c2_v.step_code,
c2_v.step_name,

#表三
'22224','22241' 这2个不知道？？？状态码里没有


#表四
步骤一没有意义啊
因为视频和其他business_flag_last,都是写入表2的流中种，2个sql都能保证自己单独有一条。
现在想一个用户只发一条,但是按mobile,occur_date进行聚合,然后取lastvalue,好像还是会有多条啊？？
group by 的last每次都会更新啊，和max一样，这个不知道干什么用的？？？？？

#表
过滤的都是视频码，多了一个 '22160'

#任务重复
6,7,8,9和2,3,4,5任务重复

区别就是3对比6，3关联的时候过滤了黑名单,6没过滤黑名单,并且3和6对于channel_type字段不同，一个映射是1，2，3一个是ths,cc,znwh
其他的基本都一样，而且都写入一个e_event_flow_stream表中。

```

### 需求梳理

```mysql
#2个部分
一个用户如果换了channel_code 去开户,那么会输出多条。

有个部分,一个打电话,一个发短信,所有一个用户是有2条的，分别在2个topic里,后端会拿分别拿做短信和电话
一个做黑名单，等客户判断,一个做非黑名单等





```



### 目前遇到的问题

```mysql
#5 时间窗口,获取窗口内部row_number排序,然后获取整条数据
select 
orderno,
spuid,
price,
create_time
from(
    select 
    orderno,
    spuid,
    price,
    create_time,
    row_number()over( partition by spuid  order by price desc , create_time desc  ) as rk
    from mock_order 
    group by TUMBLE( proctime(), INTERVAL '20' second)
)
where rk = 1

现在的问题就是group by来实现开窗,但是orderno不在groupby字段里,
目前我的需求就是按spuid聚合，然后获取价格最高，时间最近的那条数据
```









### 需求及字段整理

```mysql
需求:开户断点

requestid :唯一键，一个用户每次开户都生成一下，失败了重开也会有一条。
reductno  :是开户营业厅，如果没选，就没有，需要处理下
request_state : 审核通过，审核打回(人脸识别不请器，录语音不清晰),办理中，办理成功，申请中， 0-9都是状态,a,b,c,d还有这4个状态
       开户三方存管，就是银行卡绑定开户账号，需要银行和账户号，2方都需要操作来绑定,a,b,c,d就是这个状态。
moible_tel: 手机号

fund_account：资金账号，没有就是没开户成功
channel_code：抖音渠道，同花顺渠道，等渠道
business_flag_last:   上一步骤是什么，（开户的步骤不是固定流程的）
last_update_datetime：上一步骤完成的时间
recommender: 推荐人

#自己开户流程
输入手机号获取验证码 =>上传身份证正反面,2次分开上传,并且可以选择其他证(港澳居住证,港澳通行证,永居证)=>身份证上传后,会自动读取身份证相关信息=》自己确认也可以手动修改 =>然后点击下一步=》有个确认并提交，确认图像识别没有错别字=>我这次弹窗显示身份人已经注册过了

#要点
一个客户,任何一个断点，一天只发一次。后序可能变化


#过滤条件
request_state = 0  ; 申请中，只要是0就是客户，需要客户做的东西没做完。


#输出
一个写入kafka 给后端发短信
一个写入oracle 记录kafka推了哪些用户


#配置checkponit
默认配置文件，都是有checkpoint,觉的不要可以修改，具体看配置文件。

启停策略:9点15开启_16点15关闭等

#新需求
15分钟中断时间,上传身份中断8分钟就发消息。如果身份证发了，那么其他15分钟的就不能发了。
线上的是flink1.10版本,开发环境是1.12版本，需要去修改一下

#注意
维度表字段当on处理的时候，不能做处理的，比如split维度表字段，不可以处理，不然报错

#flinksql解析
mobile_tel,business_flag_last,channel_code,to_date(last_update_detetime)
1 ：表1按 手机号,流程到哪,渠道码,日期 开窗，按时间来 order，过滤rk=1,那么第一条来的数据永远都是1,保证了同一个状态只有一条数据。
2 ：会话窗口,超过8分钟没来消息就关闭会话， 返回最后一条
3 ：会话窗口返回的数据，通过流再做一次去重（步骤就是取窗口最早时间第一条，这样即使后面来数据了，根据rank，也是最早的是第一个，不会重复输出）


#生产把代码弄到开发
先生产打包 然后安渡  然后扔开发环境
```



### 需求注意事项

```mysql
#问题
1.只有上一个步骤,怎么判断当前是身份证认证.
1.怎么只发一次
2.怎么把（其他事15分钟,身份证是8分钟）2个分开（目前想到的是写入mysql中，这样维表关联确定是否已经发过了）

#会话窗口必然关闭问题
现在设置一个会话窗口,视频超过18分钟没来数据发短信,身份证验证超过15分钟没数据发短信
其他数据超过30分钟,发短信。
问题是视频和身份验证都正常，到最后一步提交了，整个status = 0的状态都走完事了。
最后进kafka的数据是status =1的数据,窗口必然会关闭一次，输出一个消息。

每次会话窗口关闭时，取最后一条,如果最后一条信息的提交开户申请，那么说明流程走完，那么不需要提醒了。要做where 判断



```



### 流程总结

```mysql
#表1
1 过滤了 request_status in ('0')   申请中的状态
2 按mobile_tel,business_flag_last,channel_code,to_date(last_update_detetime) 分组
  按last_update_detetime排序,然后取一条。
3 如果数据不乱序的情况下,可以保证一个手机号，一个流程状态，在同一天，只有一条数据。
  因为最早的一条才是rk为1,后序进来的数据rk只会增加。
  
#表2
1 根据表1数据，开30min会话窗口，按mobile_tel，user_id分组,30分钟关闭时,只返回最后一条数据的business_flag_last，request_no等字段，获取a2_V
2 然后关联维度表，并且过滤出business_flag_last为视频见证的流数据
```



### 1:sink_stream_crhkh_crh_wskh_mid

```mysql
#sql分析
数据源rt_crhkh_crh_wskh_userqueryextinfo,对应的kafka是:rt_crhkh_crh_wskh_userqueryextinfo
插入的表：sink_stream_crhkh_crh_wskh_mid,对应到kafka的主题：stream_crhkh_crh_wskh_mid
获取表为source_stream_crhkh_crh_wskh_mid


#步骤
按mobile_tel,business_flag_last,channel_code,to_date(last_update_detetime) 分组
按last_update_detetime排序,然后取一条。
并且过滤了 request_status in ('0')   申请中的状态

如果数据不乱序的情况下,可以保证一个手机号，一个流程状态，在同一天，只有一条数据。
因为最早的一条才是rk为1,后序进来的数据rk只会增加。

#遗漏情况
1 : 当11.59注册时,会出现跨天,这时候会输出2条,已经验证
2 : 当数据乱序的时候，后来的先到,那么也会输出多条,已验证


insert into sink_stream_crhkh_crh_wskh_mid
SELECT 
*
from (
    SELECT
    client_name,
    mobile_tel,
    branch_no,
    business_flag_last,
    channel_code,
    last_update_detetime,
    user_id,
    id_no,
    birthday,
    request_no,
    ROW_NUMBER() OVER( 
        PARTITION BY  mobile_tel,business_flag_last,channel_code,to_date(last_update_detetime) 
        order by last_update_detetime
    ) rn
    from rt_crhkh_crh_wskh_userqueryextinfo
where request_status in ('0')                         --状态：0-申请中 
) t
where t.rn = 1 ;
```



### 2:sink_stream_mot_e_event_flow_smot_cc_mid

```mysql
#sql分析
数据源是:source_stream_crhkh_crh_wskh_mid  对应到kafka的主题:stream_crhkh_crh_wskh_mid
插入表:sink_stream_mot_e_event_flow_smot_cc_mid，对应kafka主题:stream_mot_stream_account_break_ths_cc_mid
获取数据表:source_stream_mot_e_event_flow_smot_cc_mid



#总结
1 在表1中，已经保证每天，每个business_flag_last只有一条，个别情况会出现2条(乱序和跨天)。
  步骤1对于非正常开户的,当2个流程超过30分钟那么就会关闭窗口,输出条最后进入的一条信息，
  当他进入下一个流程，又超过30分钟,那么窗口会再输出一条.如果多个流程都超时会输出多条
 
2 步骤2会过滤只要视频步骤的,也就是只要视频录入之后，后序超时的,
3 步骤3用不用groupby 和row_num需要了解业务数据怎么更细，按理来说不会出现多条，都是上一流程是视频状态的
	非正常开户当多个超时30分钟，那么会输出多条,因为是按mobile_tel,channel_code分区，done_time_d排序获得rk
	只要mobile_tel,channel_code不变,那么只会输出最早超时的一条，后序超市的rk>1捕获不到


#business_flag_last状态码枚举
case
when a2_v.business_flag_last in ('12100', '22146', '22107', '22135') then'上传身份证'
when a2_v.business_flag_last in ('22145', '22111', '22224', '22241') then'个人信息修改'
when a2_v.business_flag_last in ('22123', '22106') and a2_v.branch_no = '' then'个人信息修改'
when a2_v.business_flag_last in ('22123', '22106') and a2_v.branch_no <> '' then'选择市场'
when a2_v.business_flag_last in ('22109', '22144', '22108', '22182', '22160') then'视频见证'
when a2_v.business_flag_last in ('12104', '33500') then'设置密码'
when a2_v.business_flag_last in ('22112') then'三方存管'
when a2_v.business_flag_last in ('22113', '33232', '22110') then'风险评测'
when a2_v.business_flag_last in ('22122', '22128', '22115') then'问卷回访'
end as step_name

上传身份证 : '12100', '22146', '22107', '22135'
个人信息修改: '22145', '22111', '22224', '22241','22123', '22106'
视频见证:'22109', '22144', '22108', '22182', '22160'
设置密码:'12104', '33500'
风险评测:'22113', '33232', '22110'
问卷回访:'22122', '22128', '22115'
选择市场:'22123', '22106'
三方存管:'22112'

#步骤1
数据源是第1个表
会话窗口按mobile_tel，user_id分组,30分钟关闭时,只要最后一条数据的字段，主要是看最后一条数据的business_flag_last
这里用了个自定义函数udf_time_sys(a2_v.mobile_tel, 'yyyyMMddHHmmss')


SELECT 
mobile_tel,
user_id,
lastvalue(channel_code) as channel_code,
lastvalue(client_name) as client_name,
lastvalue(branch_no) as branch_no,
cast(lastvalue(business_flag_last) as INT) as business_flag_last,
lastvalue(id_no) as id_no,
lastvalue(birthday) as birthday,
lastvalue(request_no) as request_no,
SESSION_START(PROCTIME, INTERVAL '30' MINUTE) as start_t
from source_stream_crhkh_crh_wskh_mid
group by SESSION(PROCTIME, INTERVAL '30' MINUTE),mobile_tel,user_id
作为a2_v别名表

#步骤2
获取的a2_v表,left join维度表,并且where过滤只有视频见证的business_flag_last

select
a2_v.channel_code,
a2_v.mobile_tel,
end as client_name,
cast(a2_v.business_flag_last as varchar) business_flag_last,
a2_v.start_t,
"8" as step_code,
"视频见证" as step_name
udf_time_sys(a2_v.mobile_tel, 'yyyyMMddHHmmss') as done_time_d,
from  a2_v
left join dim_crhkh_crh_channeldefine dim_channel
on a2_v.channel_code = dim_channel.channel_code
--business_flag_last只要视频见证步骤的
where a2_v.business_flag_last in ('22109', '22144', '22108', '22182', '22160') 

#步骤3
逻辑应该改为,效果是一样的
select * from(
        select 
				*
        ROW_NUMBER() OVER(PARTITION BY c2_v.mobile_tel, c2_v.channel_code order by c2_v.done_time_d) rn
        from c2_v
)
where rn = 1;

非正常开户视频内,多个超时30分钟，那么会输出多条,因为是按mobile_tel,channel_code分区，done_time_d排序获得rk
只要mobile_tel,channel_code不变,那么只会输出最早超时的一条，后序超市的rk>1捕获不到



insert into sink_stream_mot_e_event_flow_smot_cc_mid
select 
c3_v.event_id,
c3_v.event_name,
c3_v.client_name,
c3_v.channel_name,
c3_v.occur_date,
c3_v.occur_time,
from (
        select 
        DISTINCT '019-1' as event_id,
        '开户流程中断30min转化-cc（视频见证）' as event_name,
        c2_v.client_name,
        c2_v.channel_name,
        substring(c2_v.done_time_d, 1, 8) as occur_date,
        concat(substring(c2_v.done_time_d, 9, 2) as occur_time,
        ROW_NUMBER() OVER(PARTITION BY c2_v.mobile_tel, c2_v.channel_code order by c2_v.done_time_d) rn
        from c2_v
        where (c2_v.id_no is null or trim(c2_v.id_no) = '')      --过滤掉黑名单用户,黑名单用户不用发短信
        group by c2_v.client_name,
                c2_v.channel_name,
                c2_v.done_time_d,
                c2_v.birthday,
                c2_v.request_no
) c3_v
where c3_v.rn = 1;

#待确认信息
  1 步骤2会过滤只要视频步骤的,除非视频见证步骤这一步骤,后端会传入多条数据,都是这个状态,否则只有一条.
    数据来源是只有完成一个流程才会更新吗？会不会出现用户上一次完成的是视频验证步骤了，出现多条的情况。
   
  2	个人理解：录视频虽然是一个流程，但是里面分多步，比如校验身份证和本人是否一致，活体检测等，会往后台传数据，然后后台根据当前步骤看看进到哪里，
  	所以一个录视频,会有多次更新写库的操作，比如一直人脸验证和身份证不一致,一直让你眨眼
  
```



### 3:和表2插入同一个表

```mysql
#sql分析
数据源是:source_stream_crhkh_crh_wskh_mid  对应到kafka的主题:stream_crhkh_crh_wskh_mid
插入表:sink_stream_mot_e_event_flow_smot_cc_mid，对应kafka主题:stream_mot_stream_account_break_ths_cc_mid
获取数据表:source_stream_mot_e_event_flow_smot_cc_mid



#总结
非正常开户当多个超时8分钟，那么会输出多条,因为是按mobile_tel,channel_code分区，done_time_d排序获得rk
只要mobile_tel,channel_code不变,那么只会输出最早超时的一条，后序超市的rk>1捕获不到


#步骤1
1    8分钟的会话窗口,按mobile_tel，user_id分组,30分钟关闭时,只要最后一条数据的字段，主要是看最后一条数据的business_flag_last

2    在表1中，已经保证每天，每个business_flag_last只有一条，个别情况会出现2条(乱序和跨天)。
  	
  	 步骤1对于非正常开户的,当2个流程超过30分钟那么就会关闭窗口,输出条最后进入的一条信息，
  	 当他进入下一个流程，又超过30分钟,那么窗口会再输出一条.如果多个流程都超时会输出多条
  
 		 对于正常开户用户，request_status=0中,在最后一个流程结束是提交的business_flag_last是开户申请22114,因为过滤了request_status=0
  	 所以正常开户的到最后一步后,不再有后续步骤,当会话窗口关闭,输出的最后一条business_flag_last是开户申请22114。
  	 步骤2会过滤business_flag_last,把开户申请和视频流程都过滤掉了，这样就不会有数据。

SELECT 
mobile_tel,
user_id,
lastvalue(channel_code) as channel_code,
lastvalue(business_flag_last) as business_flag_last,
SESSION_START(PROCTIME, INTERVAL '8' MINUTE) as start_t
from source_stream_crhkh_crh_wskh_mid
group by SESSION(PROCTIME, INTERVAL '8' MINUTE),mobile_tel,user_id

作为临时表a2

#步骤2
business_flag_last 把视频的状态码和开户的状态码都过滤掉了
关联维度表获取字段
business_flag_last映射成step_code和step_name大类

select
a2.mobile_tel,
a2.business_flag_last ,
as step_code, --映射成大类
as step_name, --映射成大类
udf_time_sys(a2.mobile_tel, 'yyyyMMddHHmmss') as done_time_d,
dim_blacklist.id_no as id_no,
dim_basic_info.id_no as id_no2,
from a2
left join dim_crhkh_crh_channeldefine dim_channel
on a2.channel_code = dim_channel.channel_code
where a2.business_flag_last in (
'12100','22146','22107','22135', --上传身份证
'22145','22123','22106','22111', --个人信息修改
'22106', '22123',  --选择市场
'12104','33500','22112','22113','33232', --设置密码，三方存管风险评测
'22110','22122','22128','22115', --问卷回访 
'22224','22241' 这2个不知道？？？
)

#步骤3？？
这里逻辑待梳理
过滤黑名单用户
按mobile_tel,channel_code分组,取rk
groupby 字段，最后取rk一条。？？

非正常开户当多个超时8分钟，那么会输出多条,因为是按mobile_tel,channel_code分区，done_time_d排序获得rk
只要mobile_tel,channel_code不变,那么只会输出最早超时的一条，后序超市的rk>1捕获不到


insert into sink_stream_mot_e_event_flow_smot_cc_mid
select 
c3.event_id,
c3.event_name,
c3.request_no,
c3.business_name,
c3.step_code,
c3.step_name,
c3.channel_type,
from (
        select 
        DISTINCT '019-1' as event_id,
        '开户流程中断30min转化-cc（视频见证）' as event_name,
        c2.step_code,
        c2.step_name,
        c2.mobile_tel as mobile,
        c2.branch_no as branch_code,
        ROW_NUMBER() OVER(PARTITION BY c2.mobile_tel, c2.channel_code order by c2.done_time_d) rn
        from  c2
        where (c2.id_no is null or trim(c2.id_no) = '') and (c2.id_no2 is null or trim(c2.id_no2) = '')
        group by c2.client_name,
                c2.channel_name,
                c2.branch_name,
                c2.mobile_tel,
                c2.business_name,
                c2.step_code,
                c2.step_name,
                c2.channel_code,
                c2.branch_no,
                c2.done_time_d,
                c2.birthday,
                c2.request_no
) c3
where c3.rn = 1;



```

### 4:sink_stream_mot_e_event_flow_smot_cc

```mysql
#sql分析
数据源是:source_stream_mot_e_event_flow_smot_cc_mid ,kafka主题:stream_mot_stream_account_break_ths_cc_mid
插入表:sink_stream_mot_e_event_flow_smot_cc，对应kafka主题:stream_mot_stream_account_break_ths_cc
获取数据表:source_mot_stream_account_break_ths_cc

#步骤1
数据源是第二个表,按mobile,occur_date聚合group by,
udf_time_sys(a2_v.mobile_tel, 'yyyyMMddHHmmss') as done_time_d
substring(c2_v.done_time_d, 1, 8) as occur_date

因为视频和其他business_flag_last,都是写入表2的流中种，2个sql都能保证自己单独有一条。
现在想一个用户只发一条,但是按mobile,occur_date进行聚合,然后取lastvalue,好像还是会有多条啊？？
group by 的last每次都会更新啊，和max一样，这个不知道干什么用的？？？？？

数据源表,一个手机号,一辈子也就2条数据，一个30min视频的，一个8min非视频的


select  
mobile,
occur_date,
lastvalue(step_code) as step_code,
lastvalue(step_name) as step_name,
lastvalue(channel_type) as channel_type,
lastvalue(branch_code) as branch_code,
from source_stream_mot_e_event_flow_smot_cc_mid
group by mobile,occur_date


#步骤二:
有冗余,既然步骤二按mobile,occur_date分区,按occur_time排序，找最早的一条,然后rk =1
那么步骤1的group by + lastvalue(branch_code)没有意义

insert into sink_stream_mot_e_event_flow_smot_cc
select  
*
from (
        select  
        event_id,
        event_name,
        ROW_NUMBER() OVER(PARTITION BY mobile,occur_date order by occur_time) rn
        from t1 
)t1_cc
where t1_cc.rn = 1; 



```



### 5:e_event_flow_stream

```mysql
#sql分析
数据源是:source_mot_stream_account_break_ths_cc ,kafka主题:stream_mot_stream_account_break_ths_cc
插入表:e_event_flow_stream，对应oracle:MOT_DATA_UAT.E_EVENT_FLOW_STREAM


数据源表来自kafka:stream_mot_stream_account_break_ths_cc
是sink_stream_mot_e_event_flow_smot_cc写入是4表写入的

insert into e_event_flow_stream
select event_id AS EVENT_ID,
event_name     AS EVENT_NAME,
''   AS FUND_ACC_NO,
mobile  AS MOBILE,
occur_date as EVENT_DATE,
occur_time   AS EVENT_TIME, 
from source_mot_stream_account_break_ths_cc ;
```



### 6:sink_stream_mot_e_event_flow_smot_mot_mid

```mysql
#sql解析
数据源是:source_stream_crhkh_crh_wskh_mid ,对应到kafka的主题:stream_crhkh_crh_wskh_mid
插入表:sink_stream_mot_e_event_flow_smot_mot_mid，对应kafka主题:stream_mot_stream_account_break_ths_mot_mid
获取数据表:source_stream_mot_e_event_flow_smot_mot_mid



#步骤1
数据源表是 :表1
30分钟会话窗口，按mobile_tel,user_id分组, 窗口关闭，取最后一条数据

SELECT
mobile_tel,
user_id,
lastvalue(channel_code) as channel_code,
lastvalue(client_name) as client_name,
from source_stream_crhkh_crh_wskh_mid              
group by SESSION(PROCTIME,INTERVAL '30' MINUTE ),mobile_tel,user_id

别名:a2_mot_v

#步骤2
关联维表,只要视频码，还有个22160的消息

select
udf_time_sys(a2_mot_v.mobile_tel, 'yyyyMMddHHmmss')  as done_time_d,
a2_mot_v.birthday  as birthday,
a2_mot_v.request_no  
from  a2_mot_v 
left join dim_crhkh_crh_channeldefine dim_channel
on a2_mot_v.channel_code = dim_channel.channel_code
where a2_mot_v.business_flag_last in  ('22109', '22144', '22108', '22182', '22160') --22160状态吗没找到

别名:c2_mot_v 

#步骤3
PARTITION BY c2_mot.mobile_tel, c2_mot.channel_code order by c2_mot.done_time_d
mobile_tel,channel_code分组, c2_mot_v.done_time_d排序,取第一条
也就是说,第一次超时30min的,并且窗口最后一个数据是视频的,才有。后序的视频超时捕获不到rk>1

insert  into sink_stream_mot_e_event_flow_smot_mot_mid
select 
c3_mot_v.event_id,
c3_mot_v.event_name,
c3_mot_v.client_name,
c3_mot_v.last_mobilenum,
c3_mot_v.request_no,
from (
        select  
        DISTINCT  
        '019-2' as event_id,
        c2_mot_v.request_no,
        c2_mot_v.business_name,
        c2_mot_v.step_code,
        c2_mot_v.step_name,
        ROW_NUMBER() OVER(PARTITION BY c2_mot_v.mobile_tel, c2_mot_v.channel_code order by c2_mot_v.done_time_d) rn
        from  c2_mot_v                  
        group by 
        c2_mot_v.client_name,
        c2_mot_v.mobile_tel,
        c2_mot_v.channel_code,
        c2_mot_v.business_name,
        c2_mot_v.step_code,
        c2_mot_v.step_name
)c3_mot_v
where c3_mot_v.rn = 1;
```

### 7:和6插入同一个表

```mysql
#sql解析
数据源是:source_stream_crhkh_crh_wskh_mid ,对应到kafka的主题:stream_crhkh_crh_wskh_mid
插入表:sink_stream_mot_e_event_flow_smot_mot_mid，对应kafka主题:stream_mot_stream_account_break_ths_mot_mid
获取数据表:source_stream_mot_e_event_flow_smot_mot_mid


#步骤1
数据源表: 表1
8分钟会话窗口，按mobile_tel,user_id分组, 超时窗口关闭，取窗口最后一条数据

SELECT
mobile_tel,
lastvalue(channel_code) as channel_code
from source_stream_crhkh_crh_wskh_mid
group by SESSION(PROCTIME,INTERVAL '8' MINUTE ), mobile_tel, user_id

别名：a2_mot

#步骤2
business_flag_last 把视频的状态码和开户的状态码都过滤掉了
关联维度表获取字段
business_flag_last映射成step_code和step_name大类

select
a2_mot.channel_code,
a2_mot.mobile_tel,
as step_code,
as step_name,
udf_time_sys(a2_mot.mobile_tel, 'yyyyMMddHHmmss')  as done_time_d,
from a2_mot 
left join dim_crhkh_crh_channeldefine dim_channel
on a2_mot.channel_code = dim_channel.channel_code
where a2_mot.business_flag_last in (
'12100','22146','22107','22135', --上传身份证
'22145','22123','22106','22111', --个人信息修改
'22106', '22123',  --选择市场
'12104','33500','22112','22113','33232', --设置密码，三方存管风险评测
'22110','22122','22128','22115', --问卷回访 
'22224','22241' ) --这2个不知道？？？

别名:c2_mot

#步骤3
distinct + group by +row_number
PARTITION BY c2_mot.mobile_tel, c2_mot.channel_code order by c2_mot.done_time_d
只要mobile_tel，channel_code不变,只会输出最早的一条

--中断步骤为非视频见证 business_flag_last not in ('22109', '22144', '22108', '22182', '22160')
insert  into sink_stream_mot_e_event_flow_smot_mot_mid
select 
c3_mot.event_id,
c3_mot.event_name,
c3_mot.client_name,
c3_mot.last_mobilenum,
c3_mot.request_no,
from (
        select  
        DISTINCT  
        '019-2' as event_id,
        '开户流程中断8min转化-mot（非视频见证）' as event_name,
        c2_mot.client_name,
        c2_mot.request_no,
        c2_mot.business_name,
        c2_mot.step_code,
        c2_mot.step_name,
        ROW_NUMBER() OVER(PARTITION BY c2_mot.mobile_tel, c2_mot.channel_code order by c2_mot.done_time_d) rn
        from c2_mot                  
        group by 
        c2_mot.client_name,
        c2_mot.mobile_tel,
        c2_mot.step_code,
        c2_mot.step_name,
        c2_mot.done_time_d,
        c2_mot.request_no
)c3_mot
where c3_mot.rn = 1




```

### 8:sink_stream_mot_e_event_flow_smot

```mysql
#sql解析
数据源是:source_stream_mot_e_event_flow_smot_mot_mid ,kafka主题:stream_mot_stream_account_break_ths_mot_mid
插入表:sink_stream_mot_e_event_flow_smot，对应kafka主题:stream_mot_stream_account_break_ths
获取数据表:source_mot_stream_account_break_ths

select  
mobile,
occur_date,
lastvalue(event_id) as event_id
from source_stream_mot_e_event_flow_smot_mot_mid
group by mobile,occur_date

别名t2
#步骤二
--全部渠道:mot去重
insert  into sink_stream_mot_e_event_flow_smot
select  
event_id,
event_name,
client_name
from (
      select  
      event_id
      ROW_NUMBER() OVER(PARTITION BY mobile,occur_date order by occur_time) rn
      from  t2 
)t2_mot
where t2_mot.rn = 1;
                                    
                                    

```

### 9:e_event_flow_stream

```mysql
源数据表对应kafka：stream_mot_stream_account_break_ths
由sink_stream_mot_e_event_flow_smot写入kafka,是8表

-- 客户交易成功提醒事件流水记录_mot。
insert into e_event_flow_stream
select 
event_id AS EVENT_ID,
event_name  AS EVENT_NAME,
''   AS FUND_ACC_NO,
mobile    ]AS MOBILE,
occur_date as EVENT_DATE,
from source_mot_stream_account_break_ths 
```



### 10: 测试数据kafka包

```mysql
#测试发现问题
发送消息 0/2/1/11/11
当第三条时，会话窗口应该不结束,不知道为什么输出到6/7任务里kafka流多一条数据
第四条消息才应该触发10s窗口,不应该触发20秒窗口,但是触发了2/3任务
第四条消息的LAST_UPDATE_DATETIME":"2024-03-20 10:23:02
但是进入2/3任务后"occur_time":"10:23:11" 差了9秒，不知道为什么？这个occur_time的逻辑是什么
后面又触发了6/7,2/3任务，这个没关系但是4和8的过滤任务却又出现了



#jar包测试,进行生产数据
后台进行
nohup java -cp gdzh_kafka_mock-1.0-SNAPSHOT.jar KafkaMock  >> ./kafka_result.txt 2>&1 &
前台进行
java -cp gdzh_kafka_mock-1.0-SNAPSHOT.jar KafkaConsumer1



```





# 库表相关

#### 库信息

```mysql
#oracle
cfjl （大管家）


#postgresql
ods里是生产数据,不过要变更表格式



```



# 私募冷静期回访

#### 问题

```mysql
#040需求
私募冷静期回访？？不知道是不是有bug，目前没数据。逻辑在040代码里



```

# crh元数据变更评估

#### 库相关

```mysql
#用到的库
oracle的  gdods

有2个dwd的表是pg数据库的好像是,所以orcle给过滤掉了.
但是自己手动输好像可以查到dwd表
```



#### 问题

```mysql
表里有crh不管大小写的，custom_sql like "crh"


#需要去oracle的元数据去校验
gpt不准,问题太多。不过一般只会多，不会少.多的字段要去oracle元数据校验


#案例1情况
有人工排除的部分，目前只会比原来的字段表多没有的字段，去oracle获取表的字段列表，比对一下，过滤掉多的。
下面例子
SELECT (
		SELECT p1.branch_name
		FROM "STAGING"."CRHKH_CRH_USER_ALLBRANCH" p1
		WHERE p1.branch_no = t.op_branch_no
	) AS op_branch_name, t.op_branch_no, to_date(t.submit_date_time, 'yyyymmddhh24miss') AS submit_date_time
	, t.fund_account, '待审核' AS status
FROM "REALTIME"."RT_CRHKH_CRH_SBC_ACCEPTANCE" t

这里的op_branch_name,不是"REALTIME"."RT_CRHKH_CRH_SBC_ACCEPTANCE"的字段,
但是gpt分析,因为是from "REALTIME"."RT_CRHKH_CRH_SBC_ACCEPTANCE" ,所以认为是这个表的用到的字段

#案例2情况

分析这个sql,用到了哪些表,表用到了哪些字段,我只要原始字段,不要处理过的字段。
SELECT trim(mobile_tel) AS MOBILE_TEL
FROM (
	SELECT MOBILE_TEL, row_number() OVER (PARTITION BY substr(recommender, instr(recommender, '|', 1, 2) + 1, length(recommender)) ORDER BY request_datetime DESC) AS rn
	FROM realtime."RT_CRHKH_CRH_WSKH_USERQUERYEXTINFO"
	WHERE CHANNEL_CODE = '14781'
		AND request_no IN (
			SELECT a.request_no
			FROM realtime.RT_CRHKH_CRH_WSKH_CLOUDSHARERECORD a, realtime."RT_ISALES_GD_ISALES_SHARE_USER_RELATION" b
			WHERE b.C_USER_ID = substr(a.recommender, instr(a.recommender, '|', 1, 2) + 1, length(a.recommender))
				AND a.SOURCE_KIND = '2'
				AND a.CHANNEL_CODE = '14781'
				AND b.union_id = 1
		)
) t
WHERE rn = 1

这个SQL查询涉及到以下表和字段：

表：realtime."RT_CRHKH_CRH_WSKH_USERQUERYEXTINFO"
字段：MOBILE_TEL, recommender, request_datetime
表：realtime.RT_CRHKH_CRH_WSKH_CLOUDSHARERECORD
字段：request_no, recommender, SOURCE_KIND, CHANNEL_CODE
表：realtime."RT_ISALES_GD_ISALES_SHARE_USER_RELATION"
字段：C_USER_ID, union_id


回答把WHERE CHANNEL_CODE = '14781' 第一个表的CHANNEL_CODE给漏掉了。


#不加别名的调用字段,不知道是哪个表的
SELECT a.USER_ID, FUND_ACCOUNT, a.CLIENT_NAME, 'H5开户' AS open_TYPE, b.DICT_PROMPT AS ID_KIND
, ID_NO, MOBILE_TEL, d.CHANNEL_NAME, c.CHANGE_BRANCH_NAME AS branch_no, e.DICT_PROMPT AS request_status
FROM REALTIME.RT_CRHKH_CRH_WSKH_USERQUERYEXTINFO a
LEFT JOIN REALTIME.RT_CRHKH_CRH_USER_BASEDICTIONARY b ON a.ID_KIND = b.SUBENTRY
AND b.DICT_ENTRY = '1011'
LEFT JOIN dw.DWD_DIM_ORG_BRANCH c ON a.BRANCH_NO = c.BRANCH_CODE
LEFT JOIN REALTIME.RT_CRHKH_CRH_WSKH_CHANNELDEFINE d ON a.CHANNEL_CODE = d.CHANNEL_CODE
LEFT JOIN REALTIME.RT_CRHKH_CRH_USER_BASEDICTIONARY e ON a.request_status = e.SUBENTRY
AND e.DICT_ENTRY = '10100'



#案例3  3次3个答案

SELECT COALESCE(to_char(e.FUND_ACC_NO), to_char(f.FUND_ACC_NO)) AS FUND_ACC_NO
	, COALESCE(to_char(e.union_id), to_char(f.union_id)) AS UNIONID
	, CASE 
		WHEN e.FUND_ACC_NO IS NOT NULL THEN '用户体系'
		WHEN e.FUND_ACC_NO IS NULL
			AND f.FUND_ACC_NO IS NOT NULL
		THEN '乐分享'
		ELSE NULL
	END AS CHANNEL_REGISTER_SOURCE
FROM (
	SELECT DISTINCT b.FUND_ACCOUNT AS FUND_ACC_NO, a.UNION_ID AS union_id
	FROM REALTIME."RT_JYGZC_USER_USER_THIRD_PARTY_ACCOUNT" a
	LEFT JOIN REALTIME."RT_JYGZC_USER_USER_CLIENT_RELATION" b ON a.USER_ID = b.USER_ID 
		INNER JOIN realtime."RT_ISERVICE_ISE_CST_CUSTOMER" f ON a.UNION_ID = f.UNION_ID
		AND f.IS_DELETED = 0 
	WHERE a.UNION_ID IN 1
) e
	FULL JOIN (
		SELECT fund_acc_no, union_id
		FROM (
			SELECT DISTINCT e.FUND_ACCOUNT AS FUND_ACC_NO, c.unionid AS union_id, row_number() OVER (PARTITION BY c.unionid ORDER BY e.request_datetime DESC) AS rn
			FROM realtime."RT_ISALES_GD_ISALES_SHARE_CUSTOMER_USER" c
			LEFT JOIN realtime."RT_CRHKH_CRH_WSKH_CLOUDSHARERECORD" d ON to_char(c.C_USERID) = substr(d.RECOMMENDER, instr(d.RECOMMENDER, '|', -1, 1) + 1, length(d.RECOMMENDER))
				AND d.CHANNEL_CODE = '14781'
				AND d.SOURCE_KIND = '2' 
				INNER JOIN realtime."RT_CRHKH_CRH_WSKH_USERQUERYEXTINFO" e ON d.REQUEST_NO = e.REQUEST_NO
				AND e.CHANNEL_CODE = '14781' 
			WHERE c.unionid IN 1
		) t
		WHERE t.rn = 1
	) f ON e.UNION_ID = f.UNION_ID 



```



# 接口测试/导入

#### 总结

```mysql
#总结
zqtg/是数据开发团队
cfjl/大管家（财富经理）

#接口1
http://10.84.195.196:81/api/gateway/zqtg/get_cfjl_emprel_advisor   这个接口测试环境报错了，麻烦帮忙看下呢



#接口2
cfjl/opening_cust_info_yyb
cfjl/open_success_cust_info_yyb  于老师  这两个接口帮忙造点数据  营业部403的
#注意
gp和mysql不一样,mysql没有schema概念，所以dw.tb1  到mysql是dw_tb1

#查接口
登陆数据资源平台(实时和接口)  url:10.84.195.196:82
数据服务 =》API管理 =》选左上角大管家=〉
找sql，看看sql具体是什么，如果很多过滤join那么可以删除，有数据就行。在？？哪里测试跑sql呢
并看配置信息  数据源oracle  数据源名称 realtime_ods_ebscn_openaccount_api


```



#### 接口报错

```mysql
#接口1
http://10.84.195.196:81/api/gateway/zqtg/get_cfjl_emprel_advisor   这个接口测试环境报错了，麻烦帮忙看下呢

#流程
在数据开发团队项目下,搜索get_cfjl_emprel_advisor

#步骤1
找到sql:
SELECT emp_name, emp_id, cfjl_id, branch_code, branch_name 
FROM `dwdata`.`dwd_acc_cfjl_emprel_advisor` 
WHERE branch_code = ${branch_code};

#步骤二查看参数
branch_code VARCHAR = 否 否 营业部 

这个是url访问时，待的body数据样式
{ "pageNo": 1, "pageSize": null, "inFields": \{ "branch_code": "" }

#携带token
有3个方式，先记录一种,在Headers中加入 API-TOKEN： {API-TOKEN}

方式二：TOKEN加密方式
Request URL
http(s)://调用URL
Headers
API-TOKEN： {API-TOKEN}

#获取token
在上面描述里有token  :85008CC35310339E44C34D276FC54774E029CEAA7E76E6BFEDC86ECE23A146DA

#postman测试接口
API-TOKEN： {API-TOKEN}

注意没有括号
在header里输出API-TOKEN：85008CC35310339E44C34D276FC54774E029CEAA7E76E6BFEDC86ECE23A146DA

body选raw格式然后选JSON
放入{ "pageNo": 1, "pageSize": null, "inFields": \{ "branch_code": "" }



```

#### 数据导入

```mysql
#注意
gp和mysql不一样,mysql没有schema概念，所以dw.tb1  到mysql是dw_tb1

#需求
cfjl/opening_cust_info_yyb
cfjl/open_success_cust_info_yyb  于老师  这两个接口帮忙造点数据  营业部403的

#查接口
登陆数据资源平台(实时和接口)  url:10.84.195.196:82
数据服务 =》API管理 =》选左上角大管家=〉
找sql，看看sql具体是什么，如果很多过滤join那么可以删除，有数据就行。在？？哪里测试跑sql呢
并看配置信息  数据源oracle  数据源名称 realtime_ods_ebscn_openaccount_api


#sql如下:
SELECT a.client_name, a.mobile_tel, a.fund_account, a.branch_no
    , decode(a.business_flag_last, '22241', '完善用户资料', b.business_name) AS business_name
    , a.request_status, e.status
FROM "REALTIME"."RT_CRHKH_CRH_WSKH_USERQUERYEXTINFO" a
LEFT JOIN "REALTIME"."RT_CRHKH_CRH_USER_SYSBUSINESS" b ON a.business_flag_last = b.business_flag 
JOIN "REALTIME"."RT_CRHKH_CRH_USER_BASEDICTIONARY" d ON d.dict_entry = '10100'
    AND a.request_status = d.subentry 
    LEFT JOIN "REALTIME"."RT_CRHKH_CRH_WSKH_PHONEREVISIT" e ON a.REQUEST_NO = e.REQUEST_NO 
WHERE to_char(a.LAST_UPDATE_DATETIME, 'yyyymmdd') BETWEEN ${start_date} AND ${end_date}
    AND a.client_name LIKE (${client_name})
    AND a.MOBILE_TEL LIKE (${mobile_tel})
    AND a.BRANCH_NO = ${branchno}
ORDER BY a.LAST_UPDATE_DATETIME DESC;

需导的数据并不是查询数据，而是保证这个sql用到的表，都有数据就可以。这个sql会自动查表


#生产查数据 
根据ods,去生产库的交易ODS查sql，查询导出


#最后导入哪个库呢

是oracle， 数据源是realteime_ods_ebscn_opean，并且代码里用到的是realtime.tb
找到oracle库gdods,去realtime的库里，找表导入。

导入oracle看一下配置，都是先有的，没有新增的。导入出txt格式




```



# 客户成本计算

#### 库表映射

XC_DATA.CM_TKHYDJYTJ

在oracle有时候CM这个可能是分层用的,在pg_tables 进行like查询时,输入后面的就行。

然后不用管哪个pg库



#### 需知

```mysql
流水不用管,for循环不用管

#库表系统
oracle cfjl(财务经理) 用到了crm库的表
```





































